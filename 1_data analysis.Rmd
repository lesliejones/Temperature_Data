---
title: "1_data analysis"
author: "Becky"
date: "10/31/2019"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE)

library(lubridate)
library(rgdal)
library(zoo)
library(sf)
library(corrplot)
library(MuMIn)
library(ape)
library(readxl)
library(tidyverse)
```



# Anchor Data

## 8-day moving average of stream temperature data

Moving average function in CAtools which will remove NAs. We can now calculate a running mean independent of the number of data points available in each window (e.g. 8). Separately, calculate the number of non NA values in each window by replacing all non-NA values with 1 and filter on those running means with counts < 5.

Load data files from previously that only have useData == 1. 
Bind all the of data together first before calculating running means. Calculate mean daily temperatures.

```{r}

load("output/CIKdat_postQA.Rdata")
load("output/APUdat_postQA.Rdata")
load("output/KBRdat_postQA.Rdata")

temp <- as_tibble()

temp <- bind_rows(temp, CIKdatFinal) 
temp <- bind_rows(temp, KBRdatFinal) 
temp <- bind_rows(temp, APUdatFinal) 

#No NAs in temperature field so ok for calculating daily means
temp <- temp %>%
  group_by(Site, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) 


```


Use complete to fill in missing dates for all sites so that moving average is only calculating when the 8 days prior actually have data. (i.e. not going from October 2004 to April 2005).

Calculate a moving average that takes the 7 days prior and day itself and calculates a mean and removes any missing values (NAs). Add a new field that is 0 when mean temp is NA and 1 when we have a measurement. Do a rolling sum on that field over the same window so we know how many measurements were used in the rolling mean. Only keep moving averages that were calculated from 5 or more measurements! DONE!

```{r}

all.dates <- tibble(sampleDate = seq.Date(from = min(temp$sampleDate), 
                                          to = max(temp$sampleDate), by = 1)) 

temp_8day <- temp %>%
  complete(Site, all.dates) %>% 
  arrange(Site, sampleDate) %>% 
  #add a field that can be used to count non-NA values
  mutate(meanNotNA = case_when(is.na(meanT) ~ 0,
                               TRUE ~ 1)) %>% 
  group_by(Site) %>% 
  mutate(ma_mean = rollapply(meanT, width = 8, align = "right", 
                             FUN = function(x) mean(x, na.rm = TRUE), fill = NA),
         ma_sum = rollapply(meanNotNA, width = 8, align = "right", 
                            FUN = sum, fill = NA)) %>% 
  filter(!ma_sum < 5)

```

Are their missing data in temp_8day? Only for meanT, ma_mean is all filled in. When we are filling in dates and accepting moving averages with 5 or more measurements, we are extending the data series out by three days beyond the last measurement. We can delete these or accept them. For now, leave them in.

```{r}
summary(temp_8day)

temp_8day %>% filter(is.na(meanT))
```

Plot of mean temps and moving average.

```{r}

temp_8day %>% 
  filter(Site == "CIK8") %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanT)) +
  geom_line(aes(x = sampleDate-4, y = ma_mean), color = "red") +
  facet_wrap(~Site)
```


## Merge RCA ids and attributes to stream temperature data frame

RCA site ids can be used to filter the climate data actually needed for modeling.

Shifted a few sites so that they fell within the correct RCA (3 sites below the confluence of NF and SF Anchor). Also one of Sue's tributaries was shifted to be on the right stream and rca. Excel file in output folder that has site ids and RCAs linked. 

Note: the original accs_siteids applied to the KBR data weren't brought in because I had to go back to the raw data to figure out what was going on (site locations). So, create a join field that includes the contact_siteid for the KBR sites only. (contact id 2)

Also note: Some of the KBR data was not in the Anchor River watershed so they won't have rca_ids. Probably the easiest way to remove these will be when merges are made in the data analysis report (e.g. !is.na(rca_id).

```{r}

rca_join <- read_excel("Data/anchor_sites_rcas.xlsx")

rca_join <- rca_join %>% 
  mutate(Site_join = case_when(contact_id == 2 ~ contact_siteid,
                               contact_id != 2 ~ accs_siteid))

rca_join
```

Join RCA IDs to the stream temperature data frame. Remove the data for the KBR sites from the Deep and Ninikchik watersheds - 8 sites. Note that there are RCAs for the Stariski sites.

```{r}

temp.rca <- rca_join %>% 
  select(Site_join, rca_id) %>% 
  right_join(temp_8day, by = c("Site_join" = "Site")) %>% 
  rename(Site = Site_join)
  
temp.rca %>% 
  distinct(Site, rca_id) %>% 
  arrange(rca_id)

temp.rca <- temp.rca %>% 
  filter(!rca_id == 0)
```

Read in covariates associated with the reaches and RCAs in the geodatabase. Note that the reachid and the rcaids are the same.

Leslie updated the contributing area for each RCA by doing a negative buffer so we weren't grabbing cells of confluences (1/6/20). Use the rca_cont_area feature class to get the correct information for this attribute.

```{r}

fgdb <- "Data/Spatial_data/Anchor/KFHP/Geodatabases/Anchor.gdb"

# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)

# Read the feature class
rca_reach <- sf::st_read(dsn = fgdb, layer = "anch_rca_reaches")

rcas <- sf::st_read(dsn = fgdb, layer = "anchor_rcas")

contarea <- sf::st_read(dsn = fgdb, layer = "rca_cont_area")

```

Merge attributes for all to the new data frame. Use the flowacc_max value in the rca_cont_area feature class and then convert to square kilometers. (The cont_area is the flow accumulation multiplied by 5, when it should be 25.)

```{r}

temp.rca <- temp.rca %>% 
  left_join(rca_reach %>% select(reachid:slope_D), by = c("rca_id" = "reachid")) %>% 
  select(-Shape)

temp.rca <- temp.rca %>% 
  left_join(rcas %>% select(rca_id:elev_max), by = c("rca_id" = "rca_id")) %>% 
  select(-Shape)

temp.rca <- temp.rca %>% 
  left_join(contarea %>% select(rca_id, flowacc_max), by = c("rca_id" = "rca_id")) %>% 
  select(-Shape)


```

Explore if some of the elevation and slope covariates are correlated.

```{r}

temp.rca %>% 
  distinct(Z_Min, Z_Max, Z_Mean, SLength, Min_Slope, Max_Slope, Avg_Slope, length_m, slope_P, slope_D,
           elev_mean, elev_max, elev_min, flowacc_max) %>% 
  cor(.) %>% 
  corrplot.mixed(.)

```

Just keep one mean elevation, slope in percent, and contributing area. Convert flow accumulation to square meters to square kilometers for contributing area. 1 cell = 25 square meters and 1 square kilometer = 1,000,000 square meters.

See email from Leslie on 12-20-19: "use the slope_P (percent) or slope_D (degrees) for the model. Upstream Contributing Area: Maximum flow accumulation value for RCA * multiplied by cell size."

```{r}
temp.rca <- temp.rca %>% 
  select(Site, rca_id, sampleDate, meanT, ma_mean, slope_P, elev_mean, flowacc_max) %>% 
  mutate(cont_area = flowacc_max * 25 / 1000000)
```

Double check. Great! Maximum flow accumulation near the mouth is 580 km2 and HUC10 is 585.

```{r}
temp.rca %>% 
  distinct(cont_area, rca_id, Site) %>% 
  arrange(desc(cont_area))
```


## Merge daymet data to stream temperature data frame

Read in climate covariates stored in file geodatabases that were calculated for the RCAs. These are tables that have climate information linked to the RCAs.

Start with the daymet data: air, precipitation, and SWE. Leslie clarified what the date represents for DAYMET in an email on 10/18/19: "For the daymet data the date represents the mid-point of the 8-day series. For May 1st - average is 4 days prior and 3 days after."

```{r}

fgdb <- "Data/Spatial_data/Anchor/KFHP/Geodatabases/anchor_DAYMET.gdb"

# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)

# Read the feature class
daymet <- sf::st_read(dsn = fgdb, layer = "anchor_daymet")

daymet <- daymet %>% 
  mutate(date = as.Date(date)) %>% 
  select(-Field1)

daymet %>% 
  group_by(year(date)) %>% 
  summarize(min(date), max(date))


```

Merge the daymet data (i.e. air temp, swe, and precip covariates) to the stream temperature data frame so that only rca_ids and dates in the stream temperature data frame for which we have empirical data are kept. For now, remove daymet so it doesn't bog down the system (though we will need it later for prediction). 

Note: moving average of stream temperatures is currently indexed by last day of 8 day window. Since daymet is 5th day of 8-day window, need to subtract 3 days from julian date in the temp.rca data frame for merging.

Call this data frame temp.dm since it will be our model using the daymet air temperatures. (next model can be temp.lst.)

```{r}

temp.dm <- temp.rca %>% 
  mutate(date_day5 = sampleDate - 3) %>% 
  left_join(daymet, by = c("rca_id" = "rca_id", "date_day5" = "date")) 

# rm(daymet)
```

Figure out pattern in missing data. All from Jan-April and Sept-Dec.

```{r}
temp.dm %>% 
  filter(is.na(tair)) %>% 
  distinct(month(date_day5))
```

Remove fall/winter stream temps that we don't plan to model. 

```{r}
temp.dm <- temp.dm %>% 
  filter(!is.na(tair)) 
```

Add columns for week, month, and year.

```{r}
temp.dm <- temp.dm %>% 
  mutate(week = week(date_day5),
         month = month(date_day5),
         year = year(date_day5),
         day = as.numeric(format(date_day5, "%j"))) %>% 
  arrange(Site, date_day5)
  
```


Explore data.

Air temperatures by year.

```{r}
temp.dm %>% 
  ggplot() +
  geom_line(aes(x = day, y = tair)) +
  geom_line(aes(x = day, y = ma_mean), color = "red", lty = 2) +
  facet_wrap(~Site)
```

Stream temperatures by year.

```{r}
temp.dm %>% 
  ggplot() +
  geom_line(aes(x = day, y = tair, color = as.factor(year))) +
  facet_wrap(~Site)
```

Stream temperature versus air temperature with points size by precip. 

```{r}
temp.dm %>% 
  ggplot() +
  geom_point(aes(x = tair, y = ma_mean, color = prcp)) +
  facet_wrap(~Site) 

```

## Merge LST data to data frame

Read in LST data.

```{r}
fgdb <- "Data/Spatial_data/Anchor/KFHP/Geodatabases/anchor_LST.gdb"

# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)

# Read the feature class
lst <- sf::st_read(dsn = fgdb, layer = "LST_anchor_all")

lst <- lst %>% 
  mutate(date = as.Date(date)) %>% 
  select(-Field1)

```

Merge LST to the model data frame with daymet and spatial covariates. Leslie and Timm clarified what the date represents for LST in an email on 10/18/19: "The date in the LST file name represents the first day of a consecutive 8 day period.."

```{r}

temp.dm <- temp.dm %>% 
  mutate(date_day1 = date_day5 - 4) %>% 
  left_join(lst, by = c("rca_id" = "rca_id", "date_day1" = "date")) %>% 
  mutate(lst2 = LST * 0.02 - 273.15)

```


## Save data frame with empirical data, daymet, and lst.

```{r}
save(temp.dm, file = "output/temp.dm")

# load(file = "output/temp.dm.Rdata")
```



## Explore LST and daymet as covariates

See correlation between lst and daymet.
Test models of stream temp with each.

```{r}
# load(file = "output/temp.dm.Rdata")

cor(temp.dm$lst2, temp.dm$tair, use = "pairwise.complete.obs")

temp.dm %>% 
  ggplot() +
  geom_point(aes(x = tair, y = lst2)) +
  geom_abline(slope = 1, intercept = 0) +
  scale_x_continuous(limits = c(0, 25)) +
  scale_y_continuous(limits = c(0, 25)) +
  facet_wrap(~month)


```

plot LST versus stream temperature.

```{r}
temp.dm %>% 
  ggplot() +
  geom_point(aes(x = lst2, y = ma_mean, color = as.factor(month))) +
  facet_wrap(~Site) 

```



```{r}
temp.dm %>% 
  filter(!is.na(LST)) %>% 
  summarize(clm = cor(lst2, ma_mean),
            cam = cor(tair, ma_mean))


lm1 <- lm(ma_mean ~ tair, data = temp.dm %>% filter(!is.na(LST)))
lm2 <- lm(ma_mean ~ lst2, data = temp.dm %>% filter(!is.na(LST)))
AIC(lm1, lm2)

```

Try out sets of models.

```{r}

corrplot.mixed(cor(temp.dm %>% select(tair, prcp, swe, slope_P, elev_mean, cont_area)))

full.model <- lm(ma_mean ~ tair + prcp + swe + slope_P + elev_mean + cont_area, data = temp.dm,
                 na.action = "na.fail")

vif(full.model)


summary(full.model)
dredge(full.model)

```

Number of sites within each year and number of days within each year. (similar to table 1 in mcnyset)

```{r}
temp.dm %>% 
  distinct(sampleDate, year) %>% 
  count(year)

temp.dm %>% 
  distinct(Site, year) %>% 
  count(year)
```

Check for spatial autocorrelation in residuals. Following instructions here: [How can I calculate Moran's I in R?] https://stats.idre.ucla.edu/r/faq/how-can-i-calculate-morans-i-in-r/.


```{r}
temp.dm <- temp.dm %>% 
  mutate(resid.fm = resid(full.model)) 

temp.dists <- temp.dm %>% 
  left_join(rca_join %>% select(Site_join, latitude, longitude), by = c("Site" = "Site_join")) 

temp.dists <- temp.dists %>% 
  filter
  select(latitude, longitude) %>% 
  dist(.) %>% 
  as.matrix

temp.dists.inv <- 1/(temp.dists + 1)
diag(temp.dists.inv) <- 0
 
temp.dists.inv[1:5, 1:5]

temp.dists[1:50, 1:50]

Moran.I(temp.dm$resid.fm, temp.dists.inv)
```

I don't think this is working because we have very few sites, ~ 20, but 8000 observations. This may not be a meaningful thing to look at, except for when we have data from many sites on the same date.
Pick a sample date from 2007 with all 15 sites, sometime in September. Looks ok. Could loop this to calculate for a series of dates with many sites (15 or more)....

```{r}
temp.dm %>% 
  distinct(Site, sampleDate) %>% 
  count(sampleDate) %>% 
  filter(n > 14)

temp.dists <- temp.dm %>% 
  filter(sampleDate == "2007-09-20") %>% 
  left_join(rca_join %>% select(Site_join, latitude, longitude), by = c("Site" = "Site_join")) 

temp.dists <- temp.dists %>% 
  select(latitude, longitude) %>% 
  dist(.) %>% 
  as.matrix

temp.dists.inv <- 1/(temp.dists)
diag(temp.dists.inv) <- 0
 
temp.dists.inv[1:5, 1:5]

Moran.I(temp.dm %>% filter(sampleDate == "2007-09-20") %>% pull(resid.fm), temp.dists.inv)


```


## Checking for temporal autocorrelation.

We will need to fill all the missing dates back in for each site and check by Site. The function to fill in dates by group is above.

## Save CSV of all data currently available 12.20.19

```{r}
write.csv(temp.dm, "output/temp_model_data_122019.csv")
save(temp.dm, file = "output/temp_model_data.Rdata")

```



