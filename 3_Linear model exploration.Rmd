---
title: "3_Linear model exploration"
author: "Leslie Jones"
date: "1/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE)

# library(lubridate)
# library(rgdal)
# library(zoo)
# library(sf)
# library(corrplot)
library(MuMIn)
# library(ape)
library(readxl)
# library(VIF)
# library(jtools)
library(nlme)
library(car)
# library(corrplot)
# library(mgcv)
# library(visreg)
# library(GGally)
# library(Metrics)
# library(broom)
library(gridExtra)
library(tidyverse)
# library(ggeffects)
library(sjPlot)
# library(MASS)
# library(effects)



panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

```

# Add centered predictors

Centering predictors addresses structual collinearity for interactions and squared terms.

```{r}

#Note this data frame has 3-day air only.
temp.dm <- readRDS(file = "output/temp.dm.rds")


temp.dm <- temp.dm %>% 
  rename(jd = day, mean = meanT, catchment_elev_mn = rca_elev_mn) %>% 
  mutate(tair3c = scale(tair3, scale = FALSE),
         jdc = scale(jd, scale = FALSE),
         sweA1c = scale(sweA1, scale = FALSE),
         prcp5c = scale(prcp5, scale = FALSE),
         reach_slopec = scale(reach_slope, scale = FALSE),
         wetlandc = scale(Wetland, scale = FALSE),
         catchment_elev_mnc = scale(catchment_elev_mn, scale = FALSE),
         cont_areac = scale(cont_area, scale = FALSE),
         shrubc = scale(Shrub, scale = FALSE),
         riparianc = scale(Riparian, scale = FALSE),
         ca_elev_mnc = scale(ca_elev_mn, scale = FALSE))

temp.dm %>% 
  group_by(season, jd) %>% 
  summarize(mean(mean)) %>% 
  arrange(jd)

#save with scaling attributes, which are needed for predictions
temp.dm.cen <- temp.dm
save(temp.dm.cen, file = "output/temp.dm.cen.Rdata")

```

Try with sjmisc::center because having problems with plot_model.


```{r}
temp.dm <- temp.dm %>% 
  rename(jd = day, mean = meanT, catchment_elev_mn = rca_elev_mn) %>% 
  mutate(tair3c = sjmisc::center(tair3),
         jdc = sjmisc::center(jd),
         sweA1c = sjmisc::center(sweA1),
         prcp5c = sjmisc::center(prcp5),
         reach_slopec = sjmisc::center(reach_slope),
         wetlandc = sjmisc::center(Wetland),
         catchment_elev_mnc = sjmisc::center(catchment_elev_mn),
         cont_areac = sjmisc::center(cont_area),
         shrubc = sjmisc::center(Shrub),
         riparianc = sjmisc::center(Riparian),
         ca_elev_mnc = sjmisc::center(ca_elev_mn),
         season = factor(season))

```


# Example of hysteresis

make a plot showing differences in rising and fall limbs for lower anchor site.

```{r}
temp.dm %>% 
  filter(Site == "CIK14") %>% 
  ggplot(aes(x = tair3, y = mean, color = season)) +
  geom_point() +
  geom_smooth()
```

# Explore covariates to include in model

```{r, ehco=FALSE, warnings = FALSE}
#climate covariates
hist(temp$ma_mean)
hist(temp$tair)
hist(temp$prcp)
hist(temp$swe)

#then filter on distinct rcas to look at spatial covariates
temp_rca <- temp %>% distinct(rca_id, .keep_all = TRUE)

hist(temp_rca$elev_mean)
hist(temp_rca$elev_mean_ca)
hist(temp_rca$cont_area)
hist(temp_rca$slope_P)

summary(temp_rca)

#summary(temp$sweA1)
#temp %>% 
 # distinct(rca_id, year, sweA1) %>% 
 # arrange(desc(sweA1))

#hist(temp$sweA1)
```


```{r}
temp.dm %>% 
  ggplot(aes(x = tair3, y = mean, color = season)) +
  geom_point() +
  geom_smooth() +
  facet_wrap(~HUC_name)
```

```{r}

sites <- temp.dm %>% distinct(Site) %>% pull(Site)

pdf("output/Anchor air-stream temp fit by site.pdf")
for(i in sites){
  dat <- temp.dm %>% 
    filter(Site == i) 
  p <- dat %>% 
    ggplot(aes(x = tair3, y = mean, color = season)) +
    geom_point() +
    geom_smooth() +
    coord_fixed(xlim = c(0, 20), ylim = c(0, 20)) +
    theme_bw() +
    labs(title = i) 
  print(p)
}
dev.off()

```




# Pairplot of spatial covariates


```{r}

temp.dm %>% 
  distinct(reach_slope, Forest, Shrub, Wetland, Riparian, 
           catchment_elev_mn, ca_elev_mn, cont_area, sweA1) %>% 
  cor(.) %>% 
  corrplot.mixed(.)

# r = -0.93 for riparian v wetland
# r = -0.77 for wetland and forest
# r = 0.67 for catchment elevation and slope, may need to remove one. check vIF in full model.

temp.dm %>% 
  distinct(reach_slope, Shrub, Wetland, 
           catchment_elev_mn, ca_elev_mn, cont_area, sweA1) %>% 
  cor(.) %>% 
  corrplot.mixed(.)



```

Alternatively, a pairplot so can see the data points.

```{r}
temp.dm %>% 
  distinct(reach_slope, Forest, Wetland, 
           catchment_elev_mn, ca_elev_mn, cont_area, sweA1) %>% 
  pairs(upper.panel = panel.cor, lower.panel = panel.smooth)

temp.dm %>% 
  filter(!Site == "SANC-1203-M") %>% 
  distinct(reach_slope, Shrub, Wetland, 
           catchment_elev_mn, ca_elev_mn, cont_area, sweA1) %>% 
  pairs(upper.panel = panel.cor, lower.panel = panel.smooth)

temp.dm %>% 
  distinct(reach_slope) %>% 
  ggplot() +
  geom_histogram(aes(x = reach_slope))

temp.dm %>% 
  distinct(catchment_elev_mn) %>% 
  ggplot() +
  geom_histogram(aes(x = catchment_elev_mn))
  
```

Look at linear model for 2 seasons based on ascending and descending limb of stream temperature and air temperature distribution: see 2_Data and model exploration.Rmd - split is julian day 201.  Will have 2 seasons June 1 - July 20: (152-201) and july 21 - September 30: (202-303).




Check VIF for all covariates in full linear model without any random effect. Decided to drop forest and then all remaining covariates have VIF < 3.

```{r}

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 + reach_slope + 
                         Wetland + Riparian + rca_elev_mn + 
                         ca_elev_mn + cont_area,
                       data = spring_meanT, na.action = "na.fail")

vif(full.model)

#try dropping ca_elev_mn

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 + reach_slope + 
                         Wetland + Riparian + rca_elev_mn + 
                         cont_area,
                       data = spring_meanT, na.action = "na.fail")
vif(full.model)

#try dropping riparian

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 +  
                         Wetland + rca_elev_mn + reach_slope + 
                         cont_area,
                       data = spring_meanT, na.action = "na.fail")

vif(full.model)

#think about dropping reach slope, currently all VIF < 3

```

We should probably check these same VIF with the fall model.


```{r}

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 + reach_slope + 
                         Wetland + Riparian + rca_elev_mn + 
                         ca_elev_mn + cont_area,
                       data = fall_meanT, na.action = "na.fail")

vif(full.model)

#try dropping ca_elev_mn

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 + reach_slope + 
                         Wetland + Riparian + rca_elev_mn + 
                         cont_area,
                       data = fall_meanT, na.action = "na.fail")
vif(full.model)

#try dropping riparian

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 +  
                         Wetland + rca_elev_mn + reach_slope +  
                         cont_area,
                       data = fall_meanT, na.action = "na.fail")

vif(full.model)


```


Further exploration to make sure we dropped the variables with the weakest relationships to stream temperature. All looks good. Mean watershed elevation has no relation to stream temperature and reach slope and rca mean elevation are both negatively correlated to stream temperature, but elevation looks better.


```{r}

temp_meanT %>% 
  ggplot(aes(x = reach_slope, y = meanT)) +
  geom_point() 

temp_meanT %>% 
  ggplot(aes(x = ca_elev_mn, y = meanT)) +
  geom_point() 

temp_meanT %>% 
  ggplot(aes(x = rca_elev_mn, y = meanT)) +
  geom_point() 

temp_meanT %>% 
  summarize(cor(rca_elev_mn, meanT),
            cor(reach_slope, meanT),
            cor(ca_elev_mn, meanT))


```


Pairplot of covariates.

```{r}

temp %>% 
  select(tair3, day, sweA1, prcp5, Wetland, rca_elev_mn, cont_area) %>% 
  pairs(upper.panel = panel.cor, lower.panel = panel.smooth)


```

Relationship between air temperature and stream temperature by site and season. Not a real strong reason to follow up with a quadratic term for air temperature that I can see.

```{r}

spring_meanT %>% 
  ggplot(aes(x = tair3, y = meanT)) +
  geom_point() +
  facet_wrap(~Site)

fall_meanT %>% 
  ggplot(aes(x = tair3, y = meanT)) +
  geom_point() +
  facet_wrap(~Site)



```



## NOT RUN: Simple spring and fall models for testing predictions 

For testing predictions, just use air temperature and small set of spatial covariates: slope, rca mean elevation, and contributing area. Complete set inludes reach slope, wetlands, rca mean elevation, contributing area mean elevation, contributing area, and swe April 1st, 

```{r}

spring.lm <- lm(meanT ~ tair3 + reach_slope + cont_area + rca_elev_mn, data = spring_meanT)

summary(spring.lm)

vif(spring.lm)

model.rmse.spring(spring.lm)

#drop elevation since not significant

spring.lm <- lm(meanT ~ tair3 + reach_slope + cont_area, data = spring_meanT)

```

Simple fall model.

```{r}

fall.lm <- lm(meanT ~ tair3 + reach_slope + cont_area + rca_elev_mn, data = fall_meanT)

summary(fall.lm)

vif(fall.lm)

model.rmse.fall(fall.lm)

```

Save both models so they can be loaded for the predictions (script 5).

```{r}
save(spring.lm, file = "output/spring.lm.Rdata")

save(fall.lm, file = "output/fall.lm.Rdata")


```





## NOT RUN: Spatial model with HUC12 random effect versus simple fixed effects model

Compare mixed effects model to linear model to see if random effect is better.

```{r}

full.model <- lm(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
                 data = spring_meanT, na.action = "na.fail")

lme1 <- lme(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail")

AIC(lme1, full.model)

anova(lme1, full.model)
```

Mixed effects model much better than model without random effect. Move forward with model selection, but switch to ML. Add RMSE to dredge table to compare models.

```{r}

lme2 <- lme(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail", method = "ML")


lme2.dredge <- dredge(lme2, extra = c(model.rmse))
lme2.dredge %>% kable

write.csv(lme2.dredge, file = "output/lme dredge output.csv")

```
Remove wetland because small effect size - for a 10% increase in wetland cover a 0.1 deg increase in stream temp. Shrub also has a very small effect size and direction doesn't match hypthesis that shrubs provide shade: more shrub cover increases stream temperature.

Remove interaction next. Anytime it is present it changes the direction and effect size of slope.

Looks like contributing area, air temp, precip, and elevation are the most important variables and have similar RMSE to more complex models.

Try for second and third best models in lme4 dredge results. No improvement in RMSE so no justification for keeping swe or reach slope.

Try model 5, which removes precipitation from top model.

Try model 16, which removes rca mean elevation - now just air temp and contributing area.

Try model 32, which is just air temperature.... strange that coefficient goes to 1.0.

Try model 23, which is air temperature and elevation.


Check what the parameter estimates are for the random effect - ie how much does each huc shift off of the population level estimates?
Following instructions in Zuur et al. pg. 108

These seem really strange! the outlet has the warmest temperatures.

```{r}
lme4 <- lme(meanT ~ tair + cont_area, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail", method = "ML")


ranef(lme4)
```

## NOT RUN: Linear models for spring and fall

The shifts by HUC12 don't make sense so let's start all over with a regular linear model - no random effects - and go through fixed effects.

Compare spring and fall models. Also check pairwise correlations and VIF for predictors for deciding on global models.

Start with spring correlations. The difference in elevation between rca and contributing area has high correlation with a couple of covariates so just keep the mean elevations for each spatial scale (Siegel had said that differencing removed the multicollinearity, but not in this case). Shade is strongly negatively correlate to wetlands, which is interesting because it may mean those two effects can't be disentangled.

```{r}
spring_meanT %>%
  mutate(shade = shrub + forest) %>% 
  distinct(sweA1, slope_P, elev_mean, elev_mean_ca, elev_delta, cont_area, 
           wetland, shrub, forest, shade, meanT) %>% 
  cor %>% 
  corrplot.mixed(.)

temp %>% 
  distinct(rca_id, elev_mean, elev_mean_ca) %>% 
  arrange(elev_mean)
```

Drop shade because so strongly inversely correlated to wetland, which is a more important driver for the anchor. Drop elev_delta because it is pretty strongly correlated to cont_area.

```{r}
lm1 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = spring_meanT, na.action = "na.fail")

vif(lm1)

lm1.dredge <- dredge(lm1, extra = c(model.rmse.spring))  

write.csv(lm1.dredge, "output/global spring dredge.csv")

lm1.dredge

```

Compare the spring to the fall model to see if the split is needed.

```{r}

lm2 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = fall_meanT, na.action = "na.fail")

vif(lm2)

lm2.dredge <- dredge(lm2, extra = c(model.rmse.fall))
write.csv(lm2.dredge, "output/global fall dredge.csv")

```

Try a combined model for June - Sept.

```{r}

lm3 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = temp_meanT, na.action = "na.fail")

vif(lm3)

lm3.dredge <- dredge(lm3, extra = c(model.rmse))
write.csv(lm3.dredge, "output/global dredge spring and fall.csv")

```

## NOT RUN: Model sets and cross-validation

What years can we leave out for testing temporal predictions? Decided to do a leave-one-out cross-validation on years and sites.

```{r}
temp %>% 
  count(Site, year) %>% 
  spread(key = year, value = n)

temp %>% 
  count(Site)
```


For spring, start with air temperature, contributing area, and slope. Compare that model to one with each other variable added: sweA1, elevation, precipitation, and wetlands.

After first round of adding single variables, swe data improved the simple model by ~ 0.5 degrees. Add other variables to that model.

```{r}

spring.model.list <- list(meanT ~ tair3 + cont_area + rca_elev_mn,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day,
                          meanT ~ tair3 + cont_area + rca_elev_mn + sweA1,
                          meanT ~ tair3 + cont_area + rca_elev_mn + prcp5,
                          meanT ~ tair3 + cont_area + rca_elev_mn + Wetland,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day + sweA1,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day + prcp5,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day + Wetland,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day + Wetland + prcp5 + sweA1 + 
                            sweA1*day + rca_elev_mn*day,
                          meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland + 
                            rca_elev_mn*day + sweA1*Wetland)

names(spring.model.list) <- c("simple", "simple_doy", "simple_swe", "simple_prcp", "simple_wet",
                              "simple_doy_swe", "simple_doy_prcp", "simple_doy_wet", "global1", "global2")

```

Leave one out cross-validation and RMSE.
Steps:
 
* get list of sites
* loop through all sites
* leave a site out
* build the model with remaining sites
* predict for that site
* calculate RMSE for predictions against observations 
* save site name and RMSE for that site in a data frame
* summarize mean RMSE over entire leave one out CV and SD of RMSEs


```{r}

sites <- spring_meanT %>% distinct(Site) %>% pull(Site)

site.rmse <- tibble()
site.preds <- tibble()

for(i in sites){
  test.dat <- spring_meanT %>% filter(Site == i)
  train.dat <- spring_meanT %>% filter(!Site == i)
  models <- lapply(spring.model.list, function(x) lm(x, data = train.dat, na.action = "na.fail"))
  preds <- lapply(models, function(x) predict(x, newdata = test.dat))
  
  pred.dat <- tibble::enframe(unlist(preds), name = "model", value = "preds") %>% 
    mutate(model = gsub("\\..*","", model))
  test.dat.reps <- do.call("bind_rows", replicate(length(spring.model.list), 
                                 test.dat %>% select(Site, sampleDate, meanT), simplify = FALSE))
  pred.rows <- bind_cols(pred.dat, test.dat.reps)
  site.preds <- bind_rows(site.preds, pred.rows)
  
  rmse <- lapply(preds, function(x) Metrics::rmse(test.dat$meanT, x))
  newrows <- tibble::enframe(unlist(rmse), name = "model", value = "rmse") %>% 
    mutate(site = i)
  site.rmse <- bind_rows(site.rmse, newrows)
}




site.rmse %>% 
  filter(model == "simple") %>% 
  arrange(desc(rmse))

site.rmse %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse)) %>% 
  arrange(mean_rmse)

site.preds %>% 
  ggplot(aes(x = meanT, y = preds)) + 
  geom_point(shape = 1) +
  facet_wrap(~Site) +
  geom_abline(slope = 1, intercept = 0)

site.preds %>% 
  group_by(model) %>% 
  summarize(rmse = Metrics::rmse(meanT, preds)) %>% 
  arrange(desc(rmse))

```

Temporal cross-validation with spring models.

```{r}

years <- spring_meanT %>% distinct(year) %>% pull(year)

year.rmse <- tibble()

for(i in years){
  test.dat <- spring_meanT %>% filter(year == i)
  train.dat <- spring_meanT %>% filter(!year == i)
  models <- lapply(spring.model.list, function(x) lm(x, data = train.dat, na.action = "na.fail"))
  preds <- lapply(models, function(x) predict(x, newdata = test.dat))
  nobs <- nrow(test.dat)
  rmse <- lapply(preds, function(x) sqrt(sum(((x - test.dat$meanT)) ^ 2)/nobs))
  newrows <- tibble::enframe(unlist(rmse), name = "model", value = "rmse") %>% 
    mutate(year = i)
  year.rmse <- bind_rows(year.rmse, newrows)
}

year.rmse

year.rmse %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse))

year.rmse %>% 
  group_by(year) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse)) %>% 
  arrange(desc(mean_rmse))

year.swe.comp <- year.rmse %>% 
  group_by(year) %>% 
  summarize(mean_rmse = mean(rmse))

left_join(spring_meanT %>%
            group_by(sampyear) %>%
            summarize(mean_sweA1 = mean(sweA1)) %>% 
            rename(year = sampyear), year.swe.comp) %>% 
  ggplot(aes(x = mean_sweA1, y = mean_rmse)) +
  geom_point()

```

Best spring model.

```{r}

spring.lm <- lm(meanT ~ tair + slope_P + cont_area +  prcp + swe, data = spring_meanT)

summary(spring.lm)

sqrt(sum(((predict(spring.lm) - spring_meanT$meanT) ^ 2))/2235)
```


# Final summer model 9/1/20

Try converting temp.dm to data frame only and making season a factor variable to see if this will fix problems with plot_model.

```{r}
temp.dm <- as.data.frame(temp.dm)

temp.dm$season <- factor(temp.dm$season)

str(temp.dm)
```


Two small updates on 9/1: removed shrub variable because we didn't include it in the deshka and changed cutoff for timing of maximum temperatures. When writing up that summary, the results contrasted with the 201 cutoff we used previously. Evidence for early July timing (jd = 191) using both mwat and max mean daily temps.

Use centered predictors only. This reduces structural multi-collinearity for squared and interaction terms, but will simplify prediction. 

Check VIF on M1 before doing comparison of random effects. Already dropped forest due to high correlations with other variables.

```{r}

M1 <- gls(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) + sweA1c + prcp5c + reach_slopec + 
            wetlandc + catchment_elev_mnc + cont_areac + ca_elev_mnc + season +
            sweA1c:wetlandc + season:(tair3c + I(tair3c^2)), 
          data = temp.dm, na.action = "na.fail") 

vif(M1)
```
High VIF for two elevation covariates and contributing area - try dropping ca mean elevation, since that was also dropped in deshka model (will make comparisons easier).

```{r}
M1 <- gls(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) + sweA1c + prcp5c +reach_slopec + 
            wetlandc + catchment_elev_mnc + cont_areac + season +
            sweA1c:wetlandc + season:(tair3c + I(tair3c^2)), 
          data = temp.dm, na.action = "na.fail") 

vif(M1)


```
All spatial predictors now have VIF < 3. Some problems with tair/jd/season, but that was the case for the deshka as well.

Do we have data for all seasons with each huc12?

```{r}
temp.dm %>% 
  count(season, HUC_name)
```




```{r}
M2 <- lme(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) + sweA1c + prcp5c + 
            wetlandc + reach_slopec + catchment_elev_mnc + cont_areac + season +
            sweA1c:wetlandc + season:(tair3c + I(tair3c^2)), 
          random = ~ 1|HUC_name/Site, data = temp.dm, na.action = "na.fail") 

M3 <- lme(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) + sweA1c + prcp5c + 
            wetlandc + reach_slopec + catchment_elev_mnc + cont_areac + season +
            sweA1c:wetlandc + season:(tair3c + I(tair3c^2)), 
          random = ~ 1|HUC_name, data = temp.dm, na.action = "na.fail") 

data.frame(AIC = AIC(M1, M2, M3),
           rmse = c(Metrics::rmse(temp.dm$mean, predict(M1)),
           Metrics::rmse(temp.dm$mean, predict(M2)),
           Metrics::rmse(temp.dm$mean, predict(M3))))

AIC(M1) - AIC(M3)

summary(M2)


intervals(M3, which = "fixed", level = 0.85)

```

Plot model not working here, but worked in Deshka project when anchor M3 model loaded there. Move on with model selection.

```{r}
plot_model(M3, type = "pred", terms = c("tair3c [all]", "season"))

plot_model(M3, type = "pred", terms = c("jdc [all]"))
plot_model(M3, type = "pred", terms = c("prcp5c"))
plot_model(M3, type = "pred", terms = c("catchment_elev_mnc [all]"))
plot_model(M3, type = "pred", terms = c("cont_areac [all]"))

plot_model(M3, type = "pred", terms = c("wetlandc", "sweA1c"))
plot_model(M3, type = "pred", terms = c("sweA1c", "wetlandc"))

save(M3, file = "output/M3.Rdata")


M1 <- gls(mean ~ tair3c, 
          data = temp.dm, na.action = "na.fail") 
plot(predictorEffects(M1))

```

Note M2 is overfit. There is an error with intervals(M2). Error in intervals.lme(M2) : 
  cannot get confidence intervals on var-cov components: Non-positive definite approximate variance-covariance
 Consider 'which = "fixed"'. Looking up error indicates is too complex a model. Used just a random effect for watersheds.


Check multicollinearity with full model.

```{r}
vif(M3)

M3.noseason <- lme(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) + sweA1c + prcp5c + 
            wetlandc + shrubc + catchment_elev_mnc + cont_areac + 
            sweA1c:wetlandc, 
          random = ~ 1|HUC_name, data = temp.dm, na.action = "na.fail") 

vif(M3.noseason)
summary(M3.noseason)
plot_model(M3.noseason, type = "pred", terms = c("tair3c"))

plot_model(M3.noseason, type = "int")

temp.dm %>% 
  distinct(reach_slopec, wetlandc, catchment_elev_mnc, cont_areac, 
           tair3c, jdc, sweA1c, prcp5c) %>% 
  mutate(tair3c.sq = tair3c^2,
         jdc.sq = jdc^2) %>% 
  cor(.) %>% 
  corrplot.mixed(.)



```


Check assumptions.

```{r}
plot(x = predict(M3), y = resid(M3))
hist(resid(M3))


plot(x = temp.dm$tair3c, y = resid(M3))
plot(x = as.factor(temp.dm$HUC_name), y = resid(M3))
plot(x = temp.dm$jdc, y = resid(M3))
plot(x = temp.dm$sweA1c, y = resid(M3))
plot(x = temp.dm$prcp5c, y = resid(M3))
plot(x = temp.dm$cont_areac, y = resid(M3))
plot(x = temp.dm$reach_slopec, y = resid(M3))
plot(x = temp.dm$wetlandc, y = resid(M3))
plot(x = temp.dm$catchment_elev_mnc, y = resid(M3))
plot(x = as.factor(temp.dm$season), y = resid(M3))

vf3 <- varFixed(value = ~tair3)
M3.1 <- update(M3, weights = vf3)

AIC(M3, M3.1) #higher AIC, no improvement to model.

plot(x = predict(M3.1), y = resid(M3.1, type = "normalized"))


# plot(x = predict(M3), y = resid(M3, type = "normalized"))
# plot(x = temp.dm$tair3sc, y = resid(M3, type = "normalized"))
# plot(x = temp.dm$prcp5sc, y = resid(M3, type = "normalized"))
# 
# histogram(resid(M3))
# 
# plot_model(M3, type = "diag")
# plot(M3)


```

Plot observed v. fitted by site to see if there are large bias for some sites.

```{r}

temp.dm %>% 
  mutate(resid = residuals(M3, level = 1)) %>% 
  group_by(Site) %>% 
  summarize(resid_mn = mean(resid)) %>% 
  arrange(desc(resid_mn))

sites <- temp.dm %>% distinct(Site) %>% pull(Site)

pdf("output/Anchor global model obs-fitted by site.pdf")
for(i in sites){
  dat <- temp.dm %>% 
    filter(Site == i) 
  dat <- dat %>% 
    mutate(fitted = predict(M3, newdata = dat))
  rmse <- Metrics::rmse(dat$mean, dat$fitted)
  p <- dat %>% 
    ggplot() +
    geom_point(aes(x = fitted, y = mean, color = season)) +
    geom_abline(aes(slope = 1, intercept = 0)) +
    coord_fixed(xlim = c(0, 20), ylim = c(0, 20)) +
    theme_bw() +
    labs(title = i) +
    geom_text(aes(x = 10, y = 20, label = round(rmse, 2)))
  print(p)
}
dev.off()

```


Dredge and AIC.

```{r}
M3.ml <- lme(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) + sweA1c + prcp5c + 
            wetlandc + reach_slopec + catchment_elev_mnc + cont_areac + season +
            sweA1c:wetlandc + season:(tair3c + I(tair3c^2)), 
          random = ~ 1|HUC_name, data = temp.dm, na.action = "na.fail",
          method = "ML", control = lmeControl(opt = "optim"))

M3.dr <- dredge(M3.ml, fixed = "tair3c", rank = "AIC")

subset(M3.dr, delta < 10)
subset(M3.dr, cumsum(weight) <= 0.95, recalc.weights = FALSE)
subset(M3.dr, cumsum(weight) <= 0.95, recalc.weights = FALSE) %>% 
  write.csv(file = "output/model_selection_table.csv")

subset(M3.dr, 9, recalc.weights = FALSE)

tab_model(get.models(M3.dr, 9))


anchor.M3 <- lme(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) + sweA1c +  
            wetlandc + catchment_elev_mnc + cont_areac + 
            sweA1c:wetlandc, 
          random = ~ 1|HUC_name, data = temp.dm, na.action = "na.fail")


plot_model(anchor.M3, type = "pred", terms = "tair3c [all]")

save(anchor.M3, file = "output/anchor.M3.RData")

```

Try evaluating best descriptive model without JD, which is causing high VIF and see if results differ (i.e. keep interaction between season and tair^2).

```{r}
M3.2.ml <- lme(mean ~ tair3c + I(tair3c^2) + sweA1c + prcp5c + 
            wetlandc + shrubc + catchment_elev_mnc + cont_areac + season +
            sweA1c:wetlandc + season:(tair3c + I(tair3c^2)), 
          random = ~ 1|HUC_name, data = temp.dm, na.action = "na.fail",
          method = "ML", control = lmeControl(opt = "optim"))


vif(M3.2.ml)

M3.2.dr <- dredge(M3.2.ml, fixed = "tair3c", rank = "AIC")

subset(M3.2.dr, delta < 10)
subset(M3.2.dr, cumsum(weight) <= 0.95, recalc.weights = FALSE)

```



Do 10 runs of 10-fold CV on top model.
First bring in spatial cross-validation groups from ArcGIS.


```{r}

xval_join <- read_excel("Data/anchor_sites_xval.xlsx")

xval_join <- xval_join %>% 
  mutate(Site = case_when(contact_id == 2 ~ contact_siteid,
                               contact_id != 2 ~ accs_siteid))

xval_join$Site %in% (temp.dm %>% distinct(Site) %>% pull(Site))

temp.dm <- xval_join %>% 
  dplyr::select(Site, spatial_xval) %>% 
  right_join(temp.dm)

temp.dm2 <- temp.dm %>% 
  mutate(tair3c = as.vector(tair3c),
         jdc = as.vector(jdc),
         sweA1c = as.vector(sweA1c),
         prcp5c = as.vector(prcp5c),
         shrubc = as.vector(shrubc),
         wetlandc = as.vector(wetlandc),
         catchment_elev_mnc = as.vector(catchment_elev_mnc),
         cont_areac = as.vector(cont_areac))
```

Functions to calculate rmse and mae and also setting up xval grps, including temporal xval.

```{r}
rmse_calc <- function(error) { sqrt(mean(error^2)) }
mae_calc <- function(error) { mean(abs(error)) }

xval_grps <- temp.dm %>% distinct(Site, spatial_xval)
xval_grps %>% 
  arrange(spatial_xval)

```

Cross validation of top model per AIC first. This sets target for identifying most parsimonious model.

```{r}
summer_xval <- data.frame()

for(i in 1:9) {
  val_sites <- xval_grps %>% filter(spatial_xval == i) %>% pull(Site)
  train_sites <- xval_grps %>% filter(!Site %in% val_sites) %>% pull(Site)
  validation_dat <- temp.dm2 %>% 
    filter(Site %in% val_sites)
  training_dat <- temp.dm2 %>% 
    filter(Site %in% train_sites)
  fit <- lme(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) + sweA1c + 
            wetlandc + catchment_elev_mnc + cont_areac + 
            sweA1c:wetlandc, 
          random = ~ 1|HUC_name, data = training_dat, na.action = "na.fail") 
  validation_dat <- validation_dat %>% 
    mutate(preds = predict(fit, newdata = validation_dat, level = 0),
           resid = mean - preds)
  MAE <- mae_calc(validation_dat %>% pull(resid))
  RMSE <- rmse_calc(validation_dat %>% pull(resid))
  new_row <- data.frame(xval_grp = i, mae = MAE, rmse = RMSE)
  summer_xval <- bind_rows(summer_xval, new_row)
}

```
Summary of xval for top model per aic.

```{r}

summer_xval %>% 
  arrange(mae)

summer_xval %>% 
  summarize(mean(mae),
            mean(rmse))

```

Try dredging all summer models from best model per AIC, which was global model. First get list of models (unevaluated calls) and adding in dependency chains, but it appears that the interactions were always including the single terms so not needed. It was only important to add in the term for the jd^2.

```{r}

M3.op <- lme(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) + sweA1c + 
            wetlandc + catchment_elev_mnc + cont_areac + 
            sweA1c:wetlandc, 
          random = ~ 1|HUC_name, data = temp.dm, na.action = "na.fail", 
          control = lmeControl(opt = "optim"))


M3_models <- dredge(M3.op, fixed = "tair3c", eval = FALSE,
                    subset = dc(tair3c, I(tair3c^2)) && dc(jdc, I(jdc^2)) &&
                      dc(wetlandc, sweA1c:wetlandc) && dc(sweA1c, sweA1c:wetlandc))

summer_xval_dredge <- data.frame()

for(i in 1:length(M3_models)) {
  for(j in 1:9) {
    tryCatch({
      val_sites <- xval_grps %>% filter(spatial_xval == j) %>% pull(Site)
      train_sites <- sites[!sites %in% val_sites]
      validation_dat <- temp.dm2 %>% 
        filter(Site %in% val_sites)
      training_dat <- temp.dm2 %>% 
        filter(Site %in% train_sites)
      mod <- eval(M3_models[[i]])
      fit <- update(mod, data = training_dat) 
      validation_dat <- validation_dat %>% 
        mutate(preds = predict(fit, newdata = validation_dat, level = 0),
               resid = mean - preds)
      MAE <- mae_calc(validation_dat %>% pull(resid))
      RMSE <- rmse_calc(validation_dat %>% pull(resid))
      fe_ct <- nrow(summary(mod)$tTable)
      new_row <- data.frame(model_num = i, xval_grp = j, mae = MAE, rmse = RMSE, fixed_eff_ct = fe_ct)
      summer_xval_dredge <- bind_rows(summer_xval_dredge, new_row)
    }, error = function(e){cat("ERROR (model-", i, ", validation group-", j, "): ", conditionMessage(e), "\n")})
  }
}


length(M3_models)
dim(summer_xval_dredge)

```

Need a measure of model complexity for comparing xval mae and rmse. Add in number of fixed effects as random effects don't change between models.

```{r}

summer_xval_dredge

p1 <- summer_xval_dredge %>% 
  group_by(model_num, fixed_eff_ct) %>% 
  summarize(mean_mae = mean(mae), mean_rmse = mean(rmse)) %>% 
  ggplot() +
  geom_jitter(aes(x = fixed_eff_ct, y = mean_mae)) 

p2 <- summer_xval_dredge %>% 
  group_by(model_num, fixed_eff_ct) %>% 
  summarize(mean_mae = mean(mae), mean_rmse = mean(rmse)) %>% 
  ggplot() +
  geom_jitter(aes(x = fixed_eff_ct, y = mean_rmse, color = "red")) 

grid.arrange(p1,p2)
```
What are the models with fixed effects < 10 that are much better?

```{r}
summer_xval_dredge %>% 
  group_by(model_num, fixed_eff_ct) %>% 
  summarize(mean_mae = mean(mae), mean_rmse = mean(rmse)) %>% 
  filter(fixed_eff_ct < 10, mean_rmse < 1.66) %>% 
  arrange(desc(fixed_eff_ct))

summer_xval_dredge %>% 
  filter(model_num == 18) %>% 
  arrange(rmse)

M3_models[[18]]
summary(eval(M3_models[[18]]))
```

save cross-validation results.

```{r}
anchor_xval <- summer_xval_dredge

save(anchor_xval, file = "output/anchor_xval.RData")
```

Save model.

```{r}

anchor_pm <- eval(M3_models[[18]]) 
summary(anchor_pm)
tab_model(anchor_pm, digits = 4)

Metrics::rmse(temp.dm$mean, predict(anchor_pm))
rmse_calc(resid(anchor_pm))

save(anchor_pm, file = "output/anchor_pm.Rdata")

```

Try adding swe*season interaction. There were initially strong differences between the spring and fall models.

```{r}
anchor_pm2 <- update(anchor_pm, .~. + sweA1c:season)

summary(anchor_pm2)

anchor_pm2 <- update(anchor_pm, .~. + sweA1c:catchment_elev_mnc)
```



Everything below from copying from deshka script, nothing run for anchor.

Why don't we get better model results when we include the tair^2:season interaction? That would clean up all the negative residuals in the fall data (below 0 when actually they should be positive). Try running the cross-validation on the final model with just those two terms added.


```{r}
test_xval <- data.frame()

for(i in 1:10) {
  val_sites <- xval_grps %>% dplyr::filter(spatial_xval == i) %>% pull(site_ID)
  train_sites <- xval_grps %>% filter(!site_ID == val_sites) %>% pull(site_ID)
  validation_dat <- temp.dm2 %>% 
    filter(site_ID %in% val_sites)
  training_dat <- temp.dm2 %>% 
    filter(site_ID %in% train_sites)
  fit <- lme(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) +
            cont_areac + season + season:(tair3c + I(tair3c^2)), 
          random = ~ 1|HUC_name, data = training_dat, na.action = "na.fail") 
  validation_dat <- validation_dat %>% 
    mutate(preds = predict(fit, newdata = validation_dat, level = 0),
           resid = mean - preds)
  MAE <- mae_calc(validation_dat %>% pull(resid))
  RMSE <- rmse_calc(validation_dat %>% pull(resid))
  new_row <- data.frame(xval_grp = i, mae = MAE, rmse = RMSE)
  test_xval <- bind_rows(test_xval, new_row)
}

cbind(test_xval, summer_xval_dredge %>% filter(model_num == 11))

test_xval %>% 
  summarize(mean(mae), mean(rmse))

summer_xval_dredge %>% filter(model_num == 11)%>% 
  summarize(mean(mae), mean(rmse))

fit <- lme(mean ~ tair3c + I(tair3c^2) + jdc + I(jdc^2) +
             cont_areac + season + season:(tair3c + I(tair3c^2)), 
           random = ~ 1|HUC_name, data = temp.dm, na.action = "na.fail") 

cbind(temp.dm, preds = predict(fit)) %>% 
  select(jd, preds) %>% 
  arrange(desc(jd))


cbind(temp.dm, preds = predict(eval(M3_models[[11]]))) %>% 
  select(jd, preds) %>% 
  arrange(desc(jd))
```



Plotting random intercepts by sub-watershed not working yet.

```{r}
predscore <- fitted(fit)
data1<-cbind(predscore = predscore, cohort90 = mydata$cohort90, schoolid = mydata$schoolid)
data1<-unique(data1)
class(data1)
## [1] "matrix"
DF1<-data.frame(data1)
xyplot(predscore ~ cohort90, data = DF1, groups = schoolid, type = c("p", "l"), col = "blue")


pred_dat <- predict(deshka_pm, newdata = data.frame(tair3c = 5:25, jdc = 210, cont_areac = 366), level = 0)
dat <- data.frame(pred = pred_dat, tair = 5:25) 
dat %>% 
  # arrange(huc_name, tair) %>% 
  # filter(huc_name == "Lower Kroto Creek") %>% 
  ggplot(aes(x = tair, y = pred)) +
  geom_point() +
  geom_line()
```

Or just show the shifts by subwatershed.

```{r}
deshka_ri <- deshka_pm$coefficients$random$HUC_name %>% 
  as.tibble(rownames = "HUC") %>% 
  rename(Int = `(Intercept)`)

deshka_ri

deshka_int <- deshka_pm$coefficients$fixed[1] %>% 
  as.tibble() %>% 
  rename("Int" = value) %>% 
  mutate(HUC = "Pop. Int.")

fixed <- deshka_pm$coefficients$fixed[1]

bind_rows(deshka_ri, deshka_int) %>% 
  mutate(ri = case_when(HUC != "Pop. Int." ~ Int + fixed,
                        HUC == "Pop. Int." ~ Int),
         HUC = factor(HUC),
         HUC2 = relevel(HUC, "Pop. Int.")) %>% 
  ggplot() +
  geom_point(aes(x = ri, y = HUC2)) 


```

# Explore random forest and interactions

```{r}
library(randomForest)
library(rpart)


rpart(mean ~ tair3 + prcp5 + jd + sweA1 + reach_slope + Shrub + Wetland + catchment_elev_mn + ca_elev_mn + cont_area + season + HUC_name, data = temp.dm)
```




