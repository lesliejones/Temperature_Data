---
title: "3_Linear model exploration"
author: "Leslie Jones"
date: "1/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE)

library(lubridate)
library(rgdal)
library(zoo)
library(sf)
library(corrplot)
library(MuMIn)
library(ape)
library(readxl)
# library(VIF)
# library(jtools)
library(nlme)
library(car)
library(corrplot)
library(mgcv)
library(visreg)
library(GGally)
library(Metrics)
library(broom)
library(tidyverse)


load(file = "output/temp.Rdata")
# load(file = "output/tempMS.Rdata")

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

```

# Set up data frames for modeling - spring and fall.


```{r}
spring <- temp %>%
  filter(day < 202)

fall <- temp %>% 
  filter(day > 201)

spring_meanT <- spring %>% 
  filter(!is.na(meanT))

fall_meanT <- fall %>% 
  filter(!is.na(meanT))

```

We also decided to remove Stariski Creek from the model because ... we don't have enough data... not sure or prediction accuracy was low. Not sure why we removed APU1? That is located at the confluence next to Sue's long-term monitoring site. Maybe we had high prediction error for this site?

```{r}

#currently not run
spring <- temp %>%
  filter(day < 202, !Site == "APU1", !HUC_name == "Stariski Creek")

fall <- temp %>% 
  filter(day > 201, !Site == "APU1", !HUC_name == "Stariski Creek")

spring_meanT <- spring %>% 
  filter(!is.na(meanT))

fall_meanT <- fall %>% 
  filter(!is.na(meanT))

temp_meanT <- temp %>% 
  filter(!is.na(meanT), !Site == "APU1", !HUC_name == "Stariski Creek")

```


# Explore covariates to include in model

```{r, ehco=FALSE, warnings = FALSE}
#climate covariates
hist(temp$ma_mean)
hist(temp$tair)
hist(temp$prcp)
hist(temp$swe)

#then filter on distinct rcas to look at spatial covariates
temp_rca <- temp %>% distinct(rca_id, .keep_all = TRUE)

hist(temp_rca$elev_mean)
hist(temp_rca$elev_mean_ca)
hist(temp_rca$cont_area)
hist(temp_rca$slope_P)

summary(temp_rca)

#summary(temp$sweA1)
#temp %>% 
 # distinct(rca_id, year, sweA1) %>% 
 # arrange(desc(sweA1))

#hist(temp$sweA1)
```

# Pairplot of spatial covariates


```{r}

temp %>% 
  distinct(reach_slope, reach_length, Forest, Shrub, Wetland, Riparian, 
           rca_elev_min, rca_elev_max, rca_elev_mn, 
           ca_elev_mn, cont_area, elev_delta, sweA1) %>% 
  cor(.) %>% 
  corrplot.mixed(.)

```

Alternatively, a pairplot so can see the data points.

```{r}
temp %>% 
  distinct(reach_slope, reach_length, Forest, Shrub, Wetland, Riparian, 
           rca_elev_min, rca_elev_max, rca_elev_mn, 
           ca_elev_mn, cont_area, elev_delta, sweA1) %>% 
  pairs(upper.panel = panel.cor, lower.panel = panel.smooth)
  
```

Look at linear model for 2 seasons based on ascending and descending limb of stream temperature and air temperature distribution: see 2_Data and model exploration.Rmd - split is julian day 201.  Will have 2 seasons June 1 - July 20: (152-201) and july 21 - September 30: (202-303).




Check VIF for all covariates in full linear model without any random effect. Decided to drop forest and then all remaining covariates have VIF < 3.

```{r}

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 + reach_slope + 
                         Wetland + Riparian + rca_elev_mn + 
                         ca_elev_mn + cont_area,
                       data = spring_meanT, na.action = "na.fail")

vif(full.model)

#try dropping ca_elev_mn

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 + reach_slope + 
                         Wetland + Riparian + rca_elev_mn + 
                         cont_area,
                       data = spring_meanT, na.action = "na.fail")
vif(full.model)

#try dropping riparian

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 +  
                         Wetland + rca_elev_mn + reach_slope + 
                         cont_area,
                       data = spring_meanT, na.action = "na.fail")

vif(full.model)

#think about dropping reach slope, currently all VIF < 3

```

We should probably check these same VIF with the fall model.


```{r}

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 + reach_slope + 
                         Wetland + Riparian + rca_elev_mn + 
                         ca_elev_mn + cont_area,
                       data = fall_meanT, na.action = "na.fail")

vif(full.model)

#try dropping ca_elev_mn

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 + reach_slope + 
                         Wetland + Riparian + rca_elev_mn + 
                         cont_area,
                       data = fall_meanT, na.action = "na.fail")
vif(full.model)

#try dropping riparian

full.model <- lm(meanT ~ tair3 + day + sweA1 + prcp5 +  
                         Wetland + rca_elev_mn + reach_slope +  
                         cont_area,
                       data = fall_meanT, na.action = "na.fail")

vif(full.model)


```


Further exploration to make sure we dropped the variables with the weakest relationships to stream temperature. All looks good. Mean watershed elevation has no relation to stream temperature and reach slope and rca mean elevation are both negatively correlated to stream temperature, but elevation looks better.


```{r}

temp_meanT %>% 
  ggplot(aes(x = reach_slope, y = meanT)) +
  geom_point() 

temp_meanT %>% 
  ggplot(aes(x = ca_elev_mn, y = meanT)) +
  geom_point() 

temp_meanT %>% 
  ggplot(aes(x = rca_elev_mn, y = meanT)) +
  geom_point() 

temp_meanT %>% 
  summarize(cor(rca_elev_mn, meanT),
            cor(reach_slope, meanT),
            cor(ca_elev_mn, meanT))


```


Pairplot of covariates.

```{r}

temp %>% 
  select(tair3, day, sweA1, prcp5, Wetland, rca_elev_mn, cont_area) %>% 
  pairs(upper.panel = panel.cor, lower.panel = panel.smooth)


```

Relationship between air temperature and stream temperature by site and season. Not a real strong reason to follow up with a quadratic term for air temperature that I can see.

```{r}

spring_meanT %>% 
  ggplot(aes(x = tair3, y = meanT)) +
  geom_point() +
  facet_wrap(~Site)

fall_meanT %>% 
  ggplot(aes(x = tair3, y = meanT)) +
  geom_point() +
  facet_wrap(~Site)



```



## NOT RUN: Simple spring and fall models for testing predictions 

For testing predictions, just use air temperature and small set of spatial covariates: slope, rca mean elevation, and contributing area. Complete set inludes reach slope, wetlands, rca mean elevation, contributing area mean elevation, contributing area, and swe April 1st, 

```{r}

spring.lm <- lm(meanT ~ tair3 + reach_slope + cont_area + rca_elev_mn, data = spring_meanT)

summary(spring.lm)

vif(spring.lm)

model.rmse.spring(spring.lm)

#drop elevation since not significant

spring.lm <- lm(meanT ~ tair3 + reach_slope + cont_area, data = spring_meanT)

```

Simple fall model.

```{r}

fall.lm <- lm(meanT ~ tair3 + reach_slope + cont_area + rca_elev_mn, data = fall_meanT)

summary(fall.lm)

vif(fall.lm)

model.rmse.fall(fall.lm)

```

Save both models so they can be loaded for the predictions (script 5).

```{r}
save(spring.lm, file = "output/spring.lm.Rdata")

save(fall.lm, file = "output/fall.lm.Rdata")


```





## NOT RUN: Spatial model with HUC12 random effect versus simple fixed effects model

Compare mixed effects model to linear model to see if random effect is better.

```{r}

full.model <- lm(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
                 data = spring_meanT, na.action = "na.fail")

lme1 <- lme(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail")

AIC(lme1, full.model)

anova(lme1, full.model)
```

Mixed effects model much better than model without random effect. Move forward with model selection, but switch to ML. Add RMSE to dredge table to compare models.

```{r}

lme2 <- lme(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail", method = "ML")


lme2.dredge <- dredge(lme2, extra = c(model.rmse))
lme2.dredge %>% kable

write.csv(lme2.dredge, file = "output/lme dredge output.csv")

```
Remove wetland because small effect size - for a 10% increase in wetland cover a 0.1 deg increase in stream temp. Shrub also has a very small effect size and direction doesn't match hypthesis that shrubs provide shade: more shrub cover increases stream temperature.

Remove interaction next. Anytime it is present it changes the direction and effect size of slope.

Looks like contributing area, air temp, precip, and elevation are the most important variables and have similar RMSE to more complex models.

Try for second and third best models in lme4 dredge results. No improvement in RMSE so no justification for keeping swe or reach slope.

Try model 5, which removes precipitation from top model.

Try model 16, which removes rca mean elevation - now just air temp and contributing area.

Try model 32, which is just air temperature.... strange that coefficient goes to 1.0.

Try model 23, which is air temperature and elevation.


Check what the parameter estimates are for the random effect - ie how much does each huc shift off of the population level estimates?
Following instructions in Zuur et al. pg. 108

These seem really strange! the outlet has the warmest temperatures.

```{r}
lme4 <- lme(meanT ~ tair + cont_area, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail", method = "ML")


ranef(lme4)
```

## NOT RUN: Linear models for spring and fall

The shifts by HUC12 don't make sense so let's start all over with a regular linear model - no random effects - and go through fixed effects.

Compare spring and fall models. Also check pairwise correlations and VIF for predictors for deciding on global models.

Start with spring correlations. The difference in elevation between rca and contributing area has high correlation with a couple of covariates so just keep the mean elevations for each spatial scale (Siegel had said that differencing removed the multicollinearity, but not in this case). Shade is strongly negatively correlate to wetlands, which is interesting because it may mean those two effects can't be disentangled.

```{r}
spring_meanT %>%
  mutate(shade = shrub + forest) %>% 
  distinct(sweA1, slope_P, elev_mean, elev_mean_ca, elev_delta, cont_area, 
           wetland, shrub, forest, shade, meanT) %>% 
  cor %>% 
  corrplot.mixed(.)

temp %>% 
  distinct(rca_id, elev_mean, elev_mean_ca) %>% 
  arrange(elev_mean)
```

Drop shade because so strongly inversely correlated to wetland, which is a more important driver for the anchor. Drop elev_delta because it is pretty strongly correlated to cont_area.

```{r}
lm1 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = spring_meanT, na.action = "na.fail")

vif(lm1)

lm1.dredge <- dredge(lm1, extra = c(model.rmse.spring))  

write.csv(lm1.dredge, "output/global spring dredge.csv")

lm1.dredge

```

Compare the spring to the fall model to see if the split is needed.

```{r}

lm2 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = fall_meanT, na.action = "na.fail")

vif(lm2)

lm2.dredge <- dredge(lm2, extra = c(model.rmse.fall))
write.csv(lm2.dredge, "output/global fall dredge.csv")

```

Try a combined model for June - Sept.

```{r}

lm3 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = temp_meanT, na.action = "na.fail")

vif(lm3)

lm3.dredge <- dredge(lm3, extra = c(model.rmse))
write.csv(lm3.dredge, "output/global dredge spring and fall.csv")

```

## NOT RUN: Model sets and cross-validation

What years can we leave out for testing temporal predictions? Decided to do a leave-one-out cross-validation on years and sites.

```{r}
temp %>% 
  count(Site, year) %>% 
  spread(key = year, value = n)

temp %>% 
  count(Site)
```


For spring, start with air temperature, contributing area, and slope. Compare that model to one with each other variable added: sweA1, elevation, precipitation, and wetlands.

After first round of adding single variables, swe data improved the simple model by ~ 0.5 degrees. Add other variables to that model.

```{r}

spring.model.list <- list(meanT ~ tair3 + cont_area + rca_elev_mn,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day,
                          meanT ~ tair3 + cont_area + rca_elev_mn + sweA1,
                          meanT ~ tair3 + cont_area + rca_elev_mn + prcp5,
                          meanT ~ tair3 + cont_area + rca_elev_mn + Wetland,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day + sweA1,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day + prcp5,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day + Wetland,
                          meanT ~ tair3 + cont_area + rca_elev_mn + day + Wetland + prcp5 + sweA1 + 
                            sweA1*day + rca_elev_mn*day,
                          meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland + 
                            rca_elev_mn*day + sweA1*Wetland)

names(spring.model.list) <- c("simple", "simple_doy", "simple_swe", "simple_prcp", "simple_wet",
                              "simple_doy_swe", "simple_doy_prcp", "simple_doy_wet", "global1", "global2")

```

Leave one out cross-validation and RMSE.
Steps:
 
* get list of sites
* loop through all sites
* leave a site out
* build the model with remaining sites
* predict for that site
* calculate RMSE for predictions against observations 
* save site name and RMSE for that site in a data frame
* summarize mean RMSE over entire leave one out CV and SD of RMSEs


```{r}

sites <- spring_meanT %>% distinct(Site) %>% pull(Site)

site.rmse <- tibble()
site.preds <- tibble()

for(i in sites){
  test.dat <- spring_meanT %>% filter(Site == i)
  train.dat <- spring_meanT %>% filter(!Site == i)
  models <- lapply(spring.model.list, function(x) lm(x, data = train.dat, na.action = "na.fail"))
  preds <- lapply(models, function(x) predict(x, newdata = test.dat))
  
  pred.dat <- tibble::enframe(unlist(preds), name = "model", value = "preds") %>% 
    mutate(model = gsub("\\..*","", model))
  test.dat.reps <- do.call("bind_rows", replicate(length(spring.model.list), 
                                 test.dat %>% select(Site, sampleDate, meanT), simplify = FALSE))
  pred.rows <- bind_cols(pred.dat, test.dat.reps)
  site.preds <- bind_rows(site.preds, pred.rows)
  
  rmse <- lapply(preds, function(x) Metrics::rmse(test.dat$meanT, x))
  newrows <- tibble::enframe(unlist(rmse), name = "model", value = "rmse") %>% 
    mutate(site = i)
  site.rmse <- bind_rows(site.rmse, newrows)
}




site.rmse %>% 
  filter(model == "simple") %>% 
  arrange(desc(rmse))

site.rmse %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse)) %>% 
  arrange(mean_rmse)

site.preds %>% 
  ggplot(aes(x = meanT, y = preds)) + 
  geom_point(shape = 1) +
  facet_wrap(~Site) +
  geom_abline(slope = 1, intercept = 0)

site.preds %>% 
  group_by(model) %>% 
  summarize(rmse = Metrics::rmse(meanT, preds)) %>% 
  arrange(desc(rmse))

```

Temporal cross-validation with spring models.

```{r}

years <- spring_meanT %>% distinct(year) %>% pull(year)

year.rmse <- tibble()

for(i in years){
  test.dat <- spring_meanT %>% filter(year == i)
  train.dat <- spring_meanT %>% filter(!year == i)
  models <- lapply(spring.model.list, function(x) lm(x, data = train.dat, na.action = "na.fail"))
  preds <- lapply(models, function(x) predict(x, newdata = test.dat))
  nobs <- nrow(test.dat)
  rmse <- lapply(preds, function(x) sqrt(sum(((x - test.dat$meanT)) ^ 2)/nobs))
  newrows <- tibble::enframe(unlist(rmse), name = "model", value = "rmse") %>% 
    mutate(year = i)
  year.rmse <- bind_rows(year.rmse, newrows)
}

year.rmse

year.rmse %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse))

year.rmse %>% 
  group_by(year) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse)) %>% 
  arrange(desc(mean_rmse))

year.swe.comp <- year.rmse %>% 
  group_by(year) %>% 
  summarize(mean_rmse = mean(rmse))

left_join(spring_meanT %>%
            group_by(sampyear) %>%
            summarize(mean_sweA1 = mean(sweA1)) %>% 
            rename(year = sampyear), year.swe.comp) %>% 
  ggplot(aes(x = mean_sweA1, y = mean_rmse)) +
  geom_point()

```

Best spring model.

```{r}

spring.lm <- lm(meanT ~ tair + slope_P + cont_area +  prcp + swe, data = spring_meanT)

summary(spring.lm)

sqrt(sum(((predict(spring.lm) - spring_meanT$meanT) ^ 2))/2235)
```

# SPRING Backwards selection from a global model

Start by using a global model with all covariates and interactions per table with hypotheses. Drop insignificant effects and move forward. Once we have a model with all significant effects and interactions that make sense, check spatial and temporal cross-validation and drop additional covariates as needed until we reach the best model.

Backwards selection from a global model per Siegal and Volk paper. Spring model.

```{r}

spring.gl <- lm(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland + sweA1*day + 
                  rca_elev_mn*day + sweA1*Wetland + Wetland*day, data = spring_meanT)

summary(spring.gl)

```
Drop day by swe interaction.

```{r}
spring.gl <- lm(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland +  
                  rca_elev_mn*day + sweA1*Wetland + Wetland*day, data = spring_meanT)

summary(spring.gl)

```

Drop wetland by day interaction.

```{r}
spring.gl <- lm(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland +  
                  rca_elev_mn*day + sweA1*Wetland, data = spring_meanT)

summary(spring.gl)

```

Working with all data now (Stariski and APU1) - drop prcp now because no longer significant.

```{r}
spring.gl <- lm(meanT ~ tair3 + day + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland +  
                  rca_elev_mn*day + sweA1*Wetland, data = spring_meanT)

summary(spring.gl)

```

Check that remaining two interactions make sense using visualizations of their effects.

```{r}

visreg(spring.gl, "rca_elev_mn", by = "day")
visreg2d(spring.gl, "day", "rca_elev_mn", plot.type = "persp")

visreg(spring.gl, "Wetland", by = "sweA1")
visreg2d(spring.gl, "sweA1", "Wetland", plot.type = "persp")

```

Look at model cross-validation metrics (RMSE) and drop additional covariates until model train and test RMSE are no longer improved.

First create a function that spits out all of the model metrics for each model. RMSE and R2 for both training data and testing data (by site and year).

```{r}
modelMetrics <- function(model.form, model.data) {
  
  #training data r2 and rmse
  fit <- lm(model.form, data = model.data)
  train.mets <- glance(fit) %>% select(r.squared, adj.r.squared, AIC)
  train.mets$rmse <- Metrics::rmse(model.data$meanT, predict(fit))
  
  #loop for spatial cross-validation
  sites <- model.data %>% distinct(Site) %>% pull(Site)
  site.preds <- tibble()
  for(i in sites){
    test.dat <- model.data %>% filter(Site == i)
    train.dat <- model.data %>% filter(!Site == i)
    fit <- lm(model.form, data = train.dat)
    test.dat$preds <- predict(fit, newdata = test.dat)
    site.preds <- bind_rows(site.preds, test.dat %>% select(Site, sampleDate, meanT, preds))
  }
  train.mets$site.xval.rmse <- Metrics::rmse(site.preds$meanT, site.preds$preds)
  train.mets$site.xval.r2 <- summary(lm(meanT ~ preds, data = site.preds))$r.squared

  #loop for temporal cross-validation
  years <- model.data %>% distinct(year) %>% pull(year)
  year.preds <- tibble()
  for(i in years){
    test.dat <- model.data %>% filter(year == i)
    train.dat <- model.data %>% filter(!year == i)
    fit <- lm(model.form, data = train.dat)
    test.dat$preds <- predict(fit, newdata = test.dat)
    year.preds <- bind_rows(year.preds, test.dat %>% select(Site, sampleDate, meanT, preds))
  }
  train.mets$year.xval.rmse <- Metrics::rmse(year.preds$meanT, year.preds$preds)
  train.mets$year.xval.r2 <- summary(lm(meanT ~ preds, data = year.preds))$r.squared
  train.mets$model.form <- paste(model.form)[3]
  
  return(train.mets)  
}

```

Start with final model from stepwise backward selection that includes significant parameters.

```{r}

model.form1 <- formula(meanT ~ tair3 + day + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland +  
                  rca_elev_mn*day + sweA1*Wetland)

modelComp <- modelMetrics(model.form1, spring_meanT)

fit <- lm(model.form1, data = spring_meanT)
summary(fit)
visreg(fit, "day", by = "rca_elev_mn")
visreg(fit, "Wetland", by = "sweA1")

```

Try removing the swe by wetland interaction as that is the least significant. Once the interaction is removed, wetland is no longer significant, so remove that covariate as well.

```{r}

model.form2 <- formula(meanT ~ tair3 + day + sweA1 + cont_area + reach_slope + rca_elev_mn + 
                  rca_elev_mn*day)

modelComp <- bind_rows(modelComp, modelMetrics(model.form2, spring_meanT))

fit <- lm(model.form2, data = spring_meanT)
summary(fit)
visreg(fit, "reach_slope")

```

Try removing last interaction. rca elevation by day

```{r}
model.form3 <- formula(meanT ~ tair3 + day + sweA1 + cont_area + reach_slope + rca_elev_mn)

modelComp <- bind_rows(modelComp, modelMetrics(model.form3, spring_meanT))

fit <- lm(model.form3, data = spring_meanT)
summary(fit)
visreg(fit, "rca_elev_mn")

```

Try removing rca elevation, no longer significant.

```{r}
model.form4 <- formula(meanT ~ tair3 + day + sweA1 + cont_area + reach_slope)

modelComp <- bind_rows(modelComp, modelMetrics(model.form4, spring_meanT))

fit <- lm(model.form4, data = spring_meanT)
summary(fit)

```

Try removing swe

```{r}
model.form5 <- formula(meanT ~ tair3 + day + cont_area + reach_slope)

modelComp <- bind_rows(modelComp, modelMetrics(model.form5, spring_meanT))

springModelComp <- modelComp

```

Look at which sites are not predicted well with final spring model. I don't see a good reason to drop all of Stariski or the apu1 site.

```{r}
model.form <- model.form4
model.data <- spring_meanT

#loop for spatial cross-validation
  sites <- model.data %>% distinct(Site) %>% pull(Site)
  site.preds <- tibble()
  for(i in sites){
    test.dat <- model.data %>% filter(Site == i)
    train.dat <- model.data %>% filter(!Site == i)
    fit <- lm(model.form, data = train.dat)
    test.dat$preds <- predict(fit, newdata = test.dat)
    site.preds <- bind_rows(site.preds, test.dat %>% select(Site, sampleDate, meanT, preds))
  }

spring_meanT %>%
  distinct(Site, cont_area) %>% 
  right_join(site.preds) %>% 
  group_by(Site, cont_area) %>% 
  summarize(rmse = Metrics::rmse(meanT, preds)) %>% 
  ggplot(aes(x = cont_area, y = rmse)) +
  geom_point() +
  geom_text(aes(label = Site))
  
  
```


```{r}
spring_meanT %>% 
  count(Site, year) %>% 
  spread(key = year, value = n)
```

Save final model.

```{r}
spring.lm <- lm(model.form4, data = spring_meanT)

save(spring.lm, file = "output/spring.lm.Rdata")
```




# FALL Backwards selection from a global model

Start by using a global model with all covariates and interactions per table with hypotheses. Drop insignificant effects and move forward. Once we have a model with all significant effects and interactions that make sense, check spatial and temporal cross-validation and drop additional covariates as needed until we reach the best model.

Backwards selection from a global model per Siegal and Volk paper. 

```{r}

fall.gl <- lm(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland + sweA1*day + 
                  rca_elev_mn*day + sweA1*Wetland + Wetland*day, data = fall_meanT)

summary(fall.gl)

```
Drop day by wetland interaction.

```{r}
fall.gl <- lm(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland + sweA1*day + 
                  rca_elev_mn*day + sweA1*Wetland, data = fall_meanT)

summary(fall.gl)

```

Check that remaining three interactions make sense using visualizations of their effects.

```{r}

visreg(fall.gl, "rca_elev_mn", by = "day")
visreg2d(fall.gl, "rca_elev_mn","day",  plot.type = "persp")

visreg(fall.gl, "sweA1", by = "day")
visreg2d(fall.gl, "sweA1", "day", plot.type = "persp")


visreg(fall.gl, "Wetland", by = "sweA1")
visreg2d(fall.gl, "sweA1", "Wetland", plot.type = "persp")

```

Look at model cross-validation metrics (RMSE) and drop additional covariates until model train and test RMSE are no longer improved.

Start with final model from stepwise backward selection that includes significant parameters.

```{r}

model.form1 <- formula(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland + 
                         sweA1*day + rca_elev_mn*day + sweA1*Wetland)

modelComp <- modelMetrics(model.form1, fall_meanT)

summary(lm(model.form1, data = fall_meanT))

```

Try removing the swe by wetland interaction as that is the least significant. 

```{r}

model.form2 <- formula(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn + Wetland + 
                         sweA1*day + rca_elev_mn*day)

modelComp <- bind_rows(modelComp, modelMetrics(model.form2, fall_meanT))

summary(lm(model.form2, data = fall_meanT))

```

Try removing wetland next.

```{r}

model.form3 <- formula(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn +  
                         sweA1*day + rca_elev_mn*day)

modelComp <- bind_rows(modelComp, modelMetrics(model.form3, fall_meanT))

summary(lm(model.form3, data = fall_meanT))

```

Try removing each of the interactions - swe by day first.

```{r}

model.form4 <- formula(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn +  
                         rca_elev_mn*day)

modelComp <- bind_rows(modelComp, modelMetrics(model.form4, fall_meanT))

summary(lm(model.form4, data = fall_meanT))

```

Remove day x elevation interaction.

```{r}

model.form5 <- formula(meanT ~ tair3 + day + prcp5 + sweA1 + cont_area + reach_slope + rca_elev_mn)

modelComp <- bind_rows(modelComp, modelMetrics(model.form5, fall_meanT))

summary(lm(model.form5, data = fall_meanT))
visreg(lm(model.form5, data = fall_meanT), "prcp5")
```


Save final model.

```{r}
fall.lm <- lm(model.form3, data = fall_meanT)

save(fall.lm, file = "output/fall.lm.Rdata")
```


Look at which sites are not predicted well with final fall model. I don't see a good reason to drop all of Stariski or the apu1 site.

```{r}
model.form <- model.form3
model.data <- fall_meanT

#loop for spatial cross-validation
  sites <- model.data %>% distinct(Site) %>% pull(Site)
  site.preds <- tibble()
  for(i in sites){
    test.dat <- model.data %>% filter(Site == i)
    train.dat <- model.data %>% filter(!Site == i)
    fit <- lm(model.form, data = train.dat)
    test.dat$preds <- predict(fit, newdata = test.dat)
    site.preds <- bind_rows(site.preds, test.dat %>% select(Site, sampleDate, meanT, preds))
  }

fall_meanT %>%
  distinct(Site, cont_area) %>% 
  right_join(site.preds) %>% 
  group_by(Site, cont_area) %>% 
  summarize(rmse = Metrics::rmse(meanT, preds)) %>% 
  ggplot(aes(x = cont_area, y = rmse)) +
  geom_point() +
  geom_text(aes(label = Site))
  
  
```


```{r}
fall_meanT %>% 
  count(Site, year) %>% 
  spread(key = year, value = n)
```




## GAM models

Started by examining code used by Siegel and Volk in their paper and available on github.

https://github.com/SouthForkResearch/TemperatureModels_Predictive/blob/master/Model_fitting_ALL_1.R

Here is the code they used for spring and fall gam v. linear models:

```{r}

# m1gs <- (gam(AvgDailyTemp ~ s(TAVGn5dC, k = 4) +s(Tchange5C, k = 3)+s(A1Snow,by = JulianDate, k = 3)+s(catch_area, by = ae, k = 3)+s(catch_area, by = TAVGn5dC, k = 3)+s(catch_area, by = dailymeanCMS, k = 3)+s(ae, by = JulianDate, k = 3)+s(SLOPE, by = TAVGn5dC, k = 3)+s(Lake_perc, by = JulianDate, k = 3)+s(BFI, by = TAVGn5dC, k = 3)+ s(dailymeanCMS, by = TAVGn5dC, k = 3) + s(JulianDate, by = TAVGn5dC, k = 5)+ s(Echange2, by = TAVGn5dC, k = 3)+s(for_cover, by = TAVGn5dC, k = 3), data = data.day.sp))
# summary(m1gs)
# 
# #Spring linear
# m1gsL <- (lm(AvgDailyTemp ~ TAVGn5dC+Tchange5C+A1Snow*JulianDate+catch_area*ae+catch_area*TAVGn5dC+catch_area*dailymeanCMS+ae*JulianDate+SLOPE*TAVGn5dC+Lake_perc*JulianDate+dailymeanCMS*TAVGn5dC+JulianDate*TAVGn5dC+Echange2*TAVGn5dC+for_cover*TAVGn5dC, data = data.day.sp))
# summary(m1gsL)
# #fall model
# m1gf <- (gam(AvgDailyTemp ~ s(TAVGn3dC, k = 5) +s(Tchange3C, k = 3)+s(SNWD,by = TAVGn3dC, k = 3)+s(catch_area, by = ae, k = 3)+s(catch_area, by = dailymeanCMS, k = 3)+s(ae, by = SNWD, k = 3)+s(catch_area, by = TAVGn3dC, k = 3)+s(ae, by = TAVGn3dC, k = 3)+s(Lake_perc, by = JulianDate, k = 3)+ s(dailymeanCMS, by = TAVGn3dC, k = 3)+ s(BFI, by = TAVGn3dC, k = 3)+ s(Echange2, by = TAVGn3dC, k = 3)+ s(JulianDate, k = 5), data = data.day.fl))
# summary(m1gf)
# 
# m1gfL <- (lm(AvgDailyTemp ~ TAVGn3dC+Tchange3C+SNWD*TAVGn3dC+A1Snow*JulianDate+catch_area*ae+catch_area*TAVGn3dC+catch_area*dailymeanCMS+ae*JulianDate+ae*SNWD+SLOPE*TAVGn3dC+BFI*TAVGn3dC+Lake_perc*JulianDate+dailymeanCMS*TAVGn3dC+JulianDate*TAVGn3dC+Echange2*TAVGn3dC, data = data.day.fl))
# summary(m1gfL)

```

Try adding gam smoothers to the spring linear model that performed best. The main problem with this model is that we are seeing improved prediction with swe, but the effect size is in the wrong direction (+).

```{r}

spring.gam <- gam(meanT ~ s(tair, k = 3) + s(slope_P, k = 3) + s(cont_area, k = 3) +  s(prcp, k = 3) 
                  + s(swe, k = 3), data = spring_meanT)

summary(spring.gam)

sqrt(sum(((predict(spring.gam) - spring_meanT$meanT) ^ 2))/nrow(spring_meanT))

plot(spring.gam)
  

```

Compare to linear model.


```{r}

spring.lm <- lm(meanT ~ tair + slope_P + cont_area +  prcp + swe, data = spring_meanT)

summary(spring.lm)

sqrt(sum(((predict(spring.lm) - spring_meanT$meanT) ^ 2))/nrow(spring_meanT))

visreg(spring.lm, "tair")
visreg(spring.lm, "slope_P")
visreg(spring.lm, "cont_area")
visreg(spring.lm, "swe")
visreg(spring.lm, "prcp")

spring.resid <- lm(meanT ~ tair, data = spring_meanT) %>% resid
plot(spring_meanT$slope_P, spring.resid)
plot(spring_meanT$cont_area, spring.resid)
plot(spring_meanT$prcp, spring.resid)
plot(spring_meanT$swe, spring.resid)

```

Try a gam for the entire season.

```{r}
all.gam <- gam(meanT ~ s(tair, k = 5) + s(slope_P, k = 3) + s(cont_area, k = 3) +  s(prcp, k = 3) 
                  + s(sweA1, k = 3), data = temp_meanT)

summary(all.gam)

sqrt((sum((predict(all.gam) - temp_meanT$meanT) ^ 2))/nrow(temp_meanT))

plot(all.gam)
visreg(all.gam, "tair")


all.gam <- gam(meanT ~ s(tair, k = 5) + s(slope_P, k = 3) + s(cont_area, k = 3) + s(elev_mean, k = 3), data = temp_meanT)

summary(all.gam)

sqrt(sum(((predict(all.gam) - temp_meanT$meanT) ^ 2))/6049)


```

