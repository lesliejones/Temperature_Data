---
title: "3_Linear model exploration"
author: "Leslie Jones"
date: "1/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE)

library(lubridate)
library(rgdal)
library(zoo)
library(sf)
library(corrplot)
library(MuMIn)
library(ape)
library(readxl)
# library(VIF)
library(tidyverse)
# library(jtools)
library(nlme)
library(car)
library(corrplot)
library(mgcv)


load(file = "output/temp.Rdata")
load(file = "output/tempMS.Rdata")

```


###Look at distribution of covariates to include in model
```{r, ehco=FALSE, warnings = FALSE}

view(temp)
hist(temp$ma_mean)
hist(temp$tair)
hist(temp$prcp)
hist(temp$swe)

summary(temp$sweA1)
temp %>% 
  distinct(rca_id, year, sweA1) %>% 
  arrange(desc(sweA1))

hist(temp$sweA1)
```

### Look at linear model for 2 seasons based on ascending and descending limb of stream temperature and air temperature distribution: see 2_Data and model exploration.Rmd - split is julian day 201.  Will have 2 seasons June 1 - July 20: (152-201) and july 21 - September 30: (202-303)

```{r}
spring <- temp %>%
  filter(day < 202)

fall <- temp %>% 
  filter(day > 201)

spring_meanT <- spring %>% 
  filter(!is.na(meanT))

fall_meanT <- fall %>% 
  filter(!is.na(meanT))

temp_meanT <- temp %>% 
  filter(!is.na(meanT))

```

Check VIF for all covariates in full linear model without any random effect. Decided to drop forest and then all remaining covariates have VIF < 3.

```{r}
full.model.noint <- lm(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + forest, 
                 data = spring_meanT, na.action = "na.fail")

vif(full.model.noint)

full.model.noint <- lm(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub, 
                 data = spring_meanT, na.action = "na.fail")

vif(full.model.noint)

full.model.noint <- lm(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + forest, 
                 data = spring_meanT, na.action = "na.fail")

vif(full.model.noint)


```

Pairplot of covariates.

```{r}

spring_meanT %>% 
  distinct(sweM1, slope_P, elev_mean, cont_area, wetland, shrub) %>% 
  cor %>% 
  corrplot.mixed(.)

```

Compare mixed effects model to linear model to see if random effect is better.

```{r}



full.model <- lm(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
                 data = spring_meanT, na.action = "na.fail")

lme1 <- lme(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail")

AIC(lme1, full.model)

anova(lme1, full.model)
```

Mixed effects model much better than model without random effect. Move forward with model selection, but switch to ML. Add RMSE to dredge table to compare models.

```{r}

lme2 <- lme(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail", method = "ML")


model.rmse.spring <- function(x) {
  rmse <- sqrt(sum(((predict(x) - spring_meanT$meanT)) ^ 2)/3453)
  rmse
} 

model.rmse.fall <- function(x) {
  rmse <- sqrt(sum(((predict(x) - fall_meanT$meanT)) ^ 2)/4411)
  rmse
} 

model.rmse <- function(x) {
  rmse <- sqrt(sum(((predict(x) - temp_meanT$meanT)) ^ 2)/7040)
  rmse
} 

lme2.dredge <- dredge(lme2, extra = c(model.rmse))
lme2.dredge %>% kable

write.csv(lme2.dredge, file = "output/lme dredge output.csv")

```
Remove wetland because small effect size - for a 10% increase in wetland cover a 0.1 deg increase in stream temp. Shrub also has a very small effect size and direction doesn't match hypthesis that shrubs provide shade: more shrub cover increases stream temperature.

Remove interaction next. Anytime it is present it changes the direction and effect size of slope.

Looks like contributing area, air temp, precip, and elevation are the most important variables and have similar RMSE to more complex models.

Try for second and third best models in lme4 dredge results. No improvement in RMSE so no justification for keeping swe or reach slope.

Try model 5, which removes precipitation from top model.

Try model 16, which removes rca mean elevation - now just air temp and contributing area.

Try model 32, which is just air temperature.... strange that coefficient goes to 1.0.

Try model 23, which is air temperature and elevation.


Check what the parameter estimates are for the random effect - ie how much does each huc shift off of the population level estimates?
Following instructions in Zuur et al. pg. 108

These seem really strange! the outlet has the warmest temperatures.

```{r}
lme4 <- lme(meanT ~ tair + cont_area, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail", method = "ML")


ranef(lme4)
```

The shifts by HUC12 don't make sense so let's start all over with a regular linear model - no random effects - and go through fixed effects.

Compare spring and fall models. Also check pairwise correlations and VIF for predictors for deciding on global models.

Start with spring correlations. The difference in elevation between rca and contributing area has high correlation with a couple of covariates so just keep the mean elevations for each spatial scale (Siegel had said that differencing removed the multicollinearity, but not in this case). Shade is strongly negatively correlate to wetlands, which is interesting because it may mean those two effects can't be disentangled.

```{r}
spring_meanT %>%
  mutate(shade = shrub + forest) %>% 
  distinct(sweA1, slope_P, elev_mean, elev_mean_ca, elev_delta, cont_area, 
           wetland, shrub, forest, shade, meanT) %>% 
  cor %>% 
  corrplot.mixed(.)

temp %>% 
  distinct(rca_id, elev_mean, elev_mean_ca) %>% 
  arrange(elev_mean)
```

Drop shade because so strongly inversely correlated to wetland, which is a more important driver for the anchor. Drop elev_delta because it is pretty strongly correlated to cont_area.

```{r}
lm1 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = spring_meanT, na.action = "na.fail")

vif(lm1)

lm1.dredge <- dredge(lm1, extra = c(model.rmse.spring))  

write.csv(lm1.dredge, "output/global spring dredge.csv")


```

Compare the spring to the fall model to see if the split is needed.

```{r}

lm2 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = fall_meanT, na.action = "na.fail")

vif(lm2)

lm2.dredge <- dredge(lm2, extra = c(model.rmse.fall))
write.csv(lm2.dredge, "output/global fall dredge.csv")

```

Try a combined model for June - Sept.

```{r}

lm3 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = temp_meanT, na.action = "na.fail")

vif(lm3)

lm3.dredge <- dredge(lm3, extra = c(model.rmse))
write.csv(lm3.dredge, "output/global dredge spring and fall.csv")

```


What years can we leave out for testing temporal predictions?

```{r}
temp %>% 
  count(Site, year) %>% 
  spread(key = year, value = n)

temp %>% 
  count(Site)
```


Leave one out cross-validation and RMSE.
Steps:
 
* get list of sites
* loop through all sites
* leave a site out
* build the model with remaining sites
* predict for that site
* calculate RMSE for predictions against observations 
* save site name and RMSE for that site in a data frame
* summarize mean RMSE over entire leave one out CV and SD of RMSEs


```{r}

sites <- spring_meanT %>% distinct(Site) %>% pull(Site)

site.rmse <- tibble()

for(i in sites){
  test.dat <- spring_meanT %>% filter(Site == i)
  train.dat <- spring_meanT %>% filter(!Site == i)
  model <- lm(meanT ~ tair + prcp + elev_mean + cont_area,
              data = train.dat, na.action = "na.fail")
  preds <- predict(model, newdata = test.dat)
  nobs <- nrow(test.dat)
  rmse <- sqrt(sum(((preds - test.dat$meanT)) ^ 2)/nobs)
  newrow <- tibble(site = i, rmse = rmse)
  site.rmse <- bind_rows(site.rmse, newrow)
}

site.rmse

site.rmse %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse))

```

Why are some sites so poorly predicted? Do they have very little data?

It looks like the maximum number of dates in a year ~ 49, which seems low. 
For Site CIK14, dates are approx. June to late July. Did data get clipped somewhere?

```{r}
spring_meanT %>% 
  count(Site, year) %>% 
  spread(key = year, value = n)

spring_meanT %>% 
  group_by(year) %>% 
  summarize(min(sampleDate),
            max(sampleDate))

```




```{r}
chosen.model <- lm(ma_mean ~ tair  + month*tair  + slope_P + elev_mean + cont_area + forest + shrub + wetland  , data = temp_mod,
                na.action = "na.fail")

summary(chosen.model)
plot(chosen.model)
summ(chosen.model,confint = TRUE, digits = 3)

#save predicted an residual
temp_mod$predicted <- predict(chosen.model)
temp_mod$residuals <- residuals(chosen.model)
hist(temp_mod$residuals)
ggplot(data = temp_mod, aes(x= ma_mean, y=predicted, colour=year)) + geom_point()
ggplot(data = temp_mod, aes(x= ma_mean, y=predicted, colour=Site)) + geom_point()

ggplot(data = temp_mod, aes(x= ma_mean, y=residuals, color=Site)) + geom_point()
ggplot(data = temp_mod, aes(x= day, y=residuals, group=year, color=year)) + geom_point()+ facet_wrap(~Site)
ggplot(data = temp_mod, aes(x= day, y=ma_mean, group=year, color=year)) + geom_point()+ facet_wrap(~Site)

#check out high residuals
resid <- temp_mod %>% filter(residuals > 3)
ggplot(data = resid, aes(x= ma_mean, y=residuals, color=Site)) + geom_point()
ggplot(data = resid, aes(x= ma_mean, y=predicted, color=Site)) + geom_point()

#star <- temp_mod %>% filter(Site=='STAR 171 Lower') 
#ggplot(data=resid, aes(y=ma_mean, x=sampleDate)) + geom_point()

```



