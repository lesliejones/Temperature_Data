---
title: "3_Linear model exploration"
author: "Leslie Jones"
date: "1/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE)

library(lubridate)
library(rgdal)
library(zoo)
library(sf)
library(corrplot)
library(MuMIn)
library(ape)
library(readxl)
# library(VIF)
library(tidyverse)
# library(jtools)
library(nlme)
library(car)
library(corrplot)
library(mgcv)
library(visreg)


load(file = "output/temp.Rdata")
load(file = "output/tempMS.Rdata")

```
# Simple spring and fall models for testing prediction rmarkdown.

# Explore covariates to include in model

```{r, ehco=FALSE, warnings = FALSE}
#climate covariates
hist(temp$ma_mean)
hist(temp$tair)
hist(temp$prcp)
hist(temp$swe)

#then filter on distinct rcas to look at spatial covariates
temp_rca <- temp %>% distinct(rca_id, .keep_all = TRUE)

hist(temp_rca$elev_mean)
hist(temp_rca$elev_mean_ca)
hist(temp_rca$cont_area)
hist(temp_rca$slope_P)

summary(temp_rca)

#summary(temp$sweA1)
#temp %>% 
 # distinct(rca_id, year, sweA1) %>% 
 # arrange(desc(sweA1))

#hist(temp$sweA1)
```

Look at linear model for 2 seasons based on ascending and descending limb of stream temperature and air temperature distribution: see 2_Data and model exploration.Rmd - split is julian day 201.  Will have 2 seasons June 1 - July 20: (152-201) and july 21 - September 30: (202-303)

```{r}
spring <- temp %>%
  filter(day < 202, !Site == "APU1", !HUC_name == "Stariski Creek")

fall <- temp %>% 
  filter(day > 201, !Site == "APU1", !HUC_name == "Stariski Creek")

spring_meanT <- spring %>% 
  filter(!is.na(meanT))

fall_meanT <- fall %>% 
  filter(!is.na(meanT))

temp_meanT <- temp %>% 
  filter(!is.na(meanT), !Site == "APU1", !HUC_name == "Stariski Creek")

```

RMSE functions by data frame.

```{r}

model.rmse.spring <- function(x) {
  rmse <- sqrt((sum((predict(x) - spring_meanT$meanT) ^ 2))/nrow(spring_meanT))
  rmse
} 

model.rmse.fall <- function(x) {
  rmse <- sqrt((sum((predict(x) - fall_meanT$meanT) ^ 2))/nrow(fall_meanT))
  rmse
} 

model.rmse <- function(x) {
  rmse <- sqrt((sum((predict(x) - temp_meanT$meanT) ^ 2))/nrow(temp_meanT))
  rmse
} 

```


Check VIF for all covariates in full linear model without any random effect. Decided to drop forest and then all remaining covariates have VIF < 3.

```{r}
full.model.noint <- lm(meanT ~ tair + sweM1 + swe + prcp + slope_P + elev_mean + cont_area + wetland + shrub + forest, 
                 data = spring_meanT, na.action = "na.fail")

vif(full.model.noint)

full.model.noint <- lm(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub, 
                 data = spring_meanT, na.action = "na.fail")

vif(full.model.noint)

full.model.noint <- lm(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + forest, 
                 data = spring_meanT, na.action = "na.fail")

vif(full.model.noint)


```

Pairplot of covariates.

```{r}

spring_meanT %>% 
  select(swe, prcp, sweM1, slope_P, elev_mean, cont_area, wetland, shrub) %>% 
  cor %>% 
  corrplot.mixed(.)

spring_meanT %>% 
  ggplot(aes(x = swe, y = prcp)) + 
  geom_point()


```

# Spatial model with HUC12 random effect versus simple fixed effects model

Compare mixed effects model to linear model to see if random effect is better.

```{r}

full.model <- lm(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
                 data = spring_meanT, na.action = "na.fail")

lme1 <- lme(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail")

AIC(lme1, full.model)

anova(lme1, full.model)
```

Mixed effects model much better than model without random effect. Move forward with model selection, but switch to ML. Add RMSE to dredge table to compare models.

```{r}

lme2 <- lme(meanT ~ tair + sweM1 + prcp + slope_P + elev_mean + cont_area + wetland + shrub + cont_area * elev_mean, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail", method = "ML")


lme2.dredge <- dredge(lme2, extra = c(model.rmse))
lme2.dredge %>% kable

write.csv(lme2.dredge, file = "output/lme dredge output.csv")

```
Remove wetland because small effect size - for a 10% increase in wetland cover a 0.1 deg increase in stream temp. Shrub also has a very small effect size and direction doesn't match hypthesis that shrubs provide shade: more shrub cover increases stream temperature.

Remove interaction next. Anytime it is present it changes the direction and effect size of slope.

Looks like contributing area, air temp, precip, and elevation are the most important variables and have similar RMSE to more complex models.

Try for second and third best models in lme4 dredge results. No improvement in RMSE so no justification for keeping swe or reach slope.

Try model 5, which removes precipitation from top model.

Try model 16, which removes rca mean elevation - now just air temp and contributing area.

Try model 32, which is just air temperature.... strange that coefficient goes to 1.0.

Try model 23, which is air temperature and elevation.


Check what the parameter estimates are for the random effect - ie how much does each huc shift off of the population level estimates?
Following instructions in Zuur et al. pg. 108

These seem really strange! the outlet has the warmest temperatures.

```{r}
lme4 <- lme(meanT ~ tair + cont_area, 
            random = ~ 1|HUC_name, data = spring_meanT, na.action = "na.fail", method = "ML")


ranef(lme4)
```

# Linear models for spring and fall

The shifts by HUC12 don't make sense so let's start all over with a regular linear model - no random effects - and go through fixed effects.

Compare spring and fall models. Also check pairwise correlations and VIF for predictors for deciding on global models.

Start with spring correlations. The difference in elevation between rca and contributing area has high correlation with a couple of covariates so just keep the mean elevations for each spatial scale (Siegel had said that differencing removed the multicollinearity, but not in this case). Shade is strongly negatively correlate to wetlands, which is interesting because it may mean those two effects can't be disentangled.

```{r}
spring_meanT %>%
  mutate(shade = shrub + forest) %>% 
  distinct(sweA1, slope_P, elev_mean, elev_mean_ca, elev_delta, cont_area, 
           wetland, shrub, forest, shade, meanT) %>% 
  cor %>% 
  corrplot.mixed(.)

temp %>% 
  distinct(rca_id, elev_mean, elev_mean_ca) %>% 
  arrange(elev_mean)
```

Drop shade because so strongly inversely correlated to wetland, which is a more important driver for the anchor. Drop elev_delta because it is pretty strongly correlated to cont_area.

```{r}
lm1 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = spring_meanT, na.action = "na.fail")

vif(lm1)

lm1.dredge <- dredge(lm1, extra = c(model.rmse.spring))  

write.csv(lm1.dredge, "output/global spring dredge.csv")

lm1.dredge

```

Compare the spring to the fall model to see if the split is needed.

```{r}

lm2 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = fall_meanT, na.action = "na.fail")

vif(lm2)

lm2.dredge <- dredge(lm2, extra = c(model.rmse.fall))
write.csv(lm2.dredge, "output/global fall dredge.csv")

```

Try a combined model for June - Sept.

```{r}

lm3 <- lm(meanT ~ tair + sweA1 + prcp + slope_P + elev_mean + cont_area + wetland, data = temp_meanT, na.action = "na.fail")

vif(lm3)

lm3.dredge <- dredge(lm3, extra = c(model.rmse))
write.csv(lm3.dredge, "output/global dredge spring and fall.csv")

```

# Spatial and temporal cross-validations of seasonal linear models

What years can we leave out for testing temporal predictions? Decided to do a leave-one-out cross-validation on years and sites.

```{r}
temp %>% 
  count(Site, year) %>% 
  spread(key = year, value = n)

temp %>% 
  count(Site)
```


For spring, start with air temperature, contributing area, and slope. Compare that model to one with each other variable added: sweA1, elevation, precipitation, and wetlands.

After first round of adding single variables, swe data improved the simple model by ~ 0.5 degrees. Add other variables to that model.

```{r}

spring.model.list <- list(meanT ~ tair + cont_area + slope_P,
                          meanT ~ tair + cont_area + slope_P + sweA1,
                          meanT ~ tair + cont_area + slope_P + elev_mean,
                          meanT ~ tair + cont_area + slope_P + prcp,
                          meanT ~ tair + cont_area + slope_P + wetland,
                          meanT ~ tair + cont_area + slope_P + swe,
                          meanT ~ tair + cont_area + slope_P + swe + elev_mean,
                          meanT ~ tair + cont_area + slope_P + swe + prcp,
                          meanT ~ tair + cont_area + slope_P + swe + wetland,
                          meanT ~ tair + cont_area + slope_P + elev_mean + prcp + sweA1 + day + day*sweA1)

names(spring.model.list) <- c("simple", "simple_sweA1", "simple_elev", "simple_precip", "simple_wetland",
                              "simple_swe", "simple_swe_elev", "simple_swe_precip", "simple_swe_wetland",
                              "simple_dxsweA1")

```

Leave one out cross-validation and RMSE.
Steps:
 
* get list of sites
* loop through all sites
* leave a site out
* build the model with remaining sites
* predict for that site
* calculate RMSE for predictions against observations 
* save site name and RMSE for that site in a data frame
* summarize mean RMSE over entire leave one out CV and SD of RMSEs


```{r}

sites <- spring_meanT %>% distinct(Site) %>% pull(Site)

site.rmse <- tibble()
site.preds <- tibble()

for(i in sites){
  test.dat <- spring_meanT %>% filter(Site == i)
  train.dat <- spring_meanT %>% filter(!Site == i)
  models <- lapply(spring.model.list, function(x) lm(x, data = train.dat, na.action = "na.fail"))
  preds <- lapply(models, function(x) predict(x, newdata = test.dat))
  
  pred.dat <- tibble::enframe(unlist(preds), name = "model", value = "preds") %>% 
    mutate(model = gsub("\\..*","", model))
  test.dat.reps <- do.call("bind_rows", replicate(length(spring.model.list), 
                                 test.dat %>% select(Site, sampleDate, meanT), simplify = FALSE))
  pred.rows <- bind_cols(pred.dat, test.dat.reps)
  site.preds <- bind_rows(site.preds, pred.rows)
  
  nobs <- nrow(test.dat)
  rmse <- lapply(preds, function(x) sqrt(sum(((x - test.dat$meanT)) ^ 2)/nobs))
  newrows <- tibble::enframe(unlist(rmse), name = "model", value = "rmse") %>% 
    mutate(site = i)
  site.rmse <- bind_rows(site.rmse, newrows)
}




site.rmse

site.rmse %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse))

site.rmse %>% 
  group_by(site) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse)) %>% 
  arrange(desc(mean_rmse))

site.preds %>% 
  ggplot(aes(x = meanT, y = preds)) + 
  geom_point(shape = 1) +
  facet_wrap(~Site) +
  geom_abline(slope = 1, intercept = 0)
```

Temporal cross-validation with spring models.

```{r}

years <- spring_meanT %>% distinct(sampyear) %>% pull(sampyear)

year.rmse <- tibble()

for(i in years){
  test.dat <- spring_meanT %>% filter(sampyear == i)
  train.dat <- spring_meanT %>% filter(!sampyear == i)
  models <- lapply(spring.model.list, function(x) lm(x, data = train.dat, na.action = "na.fail"))
  preds <- lapply(models, function(x) predict(x, newdata = test.dat))
  nobs <- nrow(test.dat)
  rmse <- lapply(preds, function(x) sqrt(sum(((x - test.dat$meanT)) ^ 2)/nobs))
  newrows <- tibble::enframe(unlist(rmse), name = "model", value = "rmse") %>% 
    mutate(year = i)
  year.rmse <- bind_rows(year.rmse, newrows)
}

year.rmse

year.rmse %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse))

year.rmse %>% 
  group_by(year) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse)) %>% 
  arrange(desc(mean_rmse))

year.swe.comp <- year.rmse %>% 
  group_by(year) %>% 
  summarize(mean_rmse = mean(rmse))

left_join(spring_meanT %>%
            group_by(sampyear) %>%
            summarize(mean_sweA1 = mean(sweA1)) %>% 
            rename(year = sampyear), year.swe.comp) %>% 
  ggplot(aes(x = mean_sweA1, y = mean_rmse)) +
  geom_point()

```

Best spring model.

```{r}

spring.lm <- lm(meanT ~ tair + slope_P + cont_area +  prcp + swe, data = spring_meanT)

summary(spring.lm)

sqrt(sum(((predict(spring.lm) - spring_meanT$meanT) ^ 2))/2235)


```

spring global model. What is significant?

```{r}
spring.lm <- lm(meanT ~ tair + slope_P + elev_mean + cont_area +  prcp + swe + sweA1 + wetland, data = spring_meanT)
summary(spring.lm)
sqrt(sum(((predict(spring.lm) - spring_meanT$meanT) ^ 2))/2235)

spring.lm <- lm(meanT ~ tair + slope_P + elev_mean + cont_area +  prcp + swe + sweA1, data = spring_meanT)
sqrt(sum(((predict(spring.lm) - spring_meanT$meanT) ^ 2))/2235)
summary(spring.lm)

spring.lm <- lm(meanT ~ tair + slope_P + elev_mean + cont_area +  prcp + sweA1, data = spring_meanT)
sqrt(sum(((predict(spring.lm) - spring_meanT$meanT) ^ 2))/2235)
summary(spring.lm)

spring.lm <- lm(meanT ~ tair + slope_P + cont_area +  prcp + sweA1, data = spring_meanT)
sqrt(sum(((predict(spring.lm) - spring_meanT$meanT) ^ 2))/2235)
summary(spring.lm)

```

Cross validation rmse for fall models. Start with air temperature, contributing area, elevation, and slope. Compare that model to one with each other variable added: sweA1, precipitation, and wetlands.

After first round, adding in SWE improved model RMSE by 0.3 degrees. Take that model and add in precip and wetlands to see if further improvement is possible.

```{r}

fall.model.list <- list(meanT ~ tair + cont_area + slope_P + elev_mean,
                        meanT ~ tair + cont_area + slope_P + elev_mean + sweA1,
                        meanT ~ tair + cont_area + slope_P + elev_mean + prcp,
                        meanT ~ tair + cont_area + slope_P + elev_mean + wetland,
                        meanT ~ tair + cont_area + slope_P + elev_mean + swe,
                        meanT ~ tair + cont_area + slope_P + elev_mean + sweA1 + prcp,
                        meanT ~ tair + cont_area + slope_P + elev_mean + sweA1 + wetland,
                        meanT ~ tair + cont_area + slope_P + elev_mean + sweA1 + day + day*sweA1)

names(fall.model.list) <- c("simple", "simple_sweA1", "simple_precip", "simple_wetland", "simple_swe",
                            "simple_sweA1_precip", "simple_sweA1_wetland", "simple_dxsweA1")

```

Spatial cross-validation of fall models.

```{r}

sites <- fall_meanT %>% distinct(Site) %>% pull(Site)

site.rmse.fall <- tibble()

for(i in sites){
  test.dat <- fall_meanT %>% filter(Site == i)
  train.dat <- fall_meanT %>% filter(!Site == i)
  models <- lapply(fall.model.list, function(x) lm(x, data = train.dat, na.action = "na.fail"))
  preds <- lapply(models, function(x) predict(x, newdata = test.dat))
  nobs <- nrow(test.dat)
  rmse <- lapply(preds, function(x) sqrt(sum(((x - test.dat$meanT)) ^ 2)/nobs))
  newrows <- tibble::enframe(unlist(rmse), name = "model", value = "rmse") %>% 
    mutate(site = i)
  site.rmse.fall <- bind_rows(site.rmse.fall, newrows)
}

site.rmse.fall

site.rmse.fall %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse))

site.rmse.fall %>% 
  group_by(site) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse)) %>% 
  arrange(desc(mean_rmse))
```

Temporal cross-validation with fall models.

```{r}

years <- fall_meanT %>% distinct(sampyear) %>% pull(sampyear)

year.rmse.fall <- tibble()

for(i in years){
  test.dat <- fall_meanT %>% filter(sampyear == i)
  train.dat <- fall_meanT %>% filter(!sampyear == i)
  models <- lapply(fall.model.list, function(x) lm(x, data = train.dat, na.action = "na.fail"))
  preds <- lapply(models, function(x) predict(x, newdata = test.dat))
  nobs <- nrow(test.dat)
  rmse <- lapply(preds, function(x) sqrt(sum(((x - test.dat$meanT)) ^ 2)/nobs))
  newrows <- tibble::enframe(unlist(rmse), name = "model", value = "rmse") %>% 
    mutate(year = i)
  year.rmse.fall <- bind_rows(year.rmse.fall, newrows)
}

year.rmse.fall

year.rmse.fall %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse))

year.rmse.fall %>% 
  group_by(year) %>% 
  summarize(mean_rmse = mean(rmse),
            sd_rmse = sd(rmse)) %>% 
  arrange(desc(mean_rmse))
```


Fall best model - meanT ~ tair + cont_area + slope_P + elev_mean + sweA1 + prcp. Look to see if parameter coefficients make sense now that Stariski is dropped.

```{r}

fall.lm <- lm(meanT ~ tair + cont_area + slope_P + elev_mean + sweA1 + prcp, data = fall_meanT)

summary(fall.lm)

```




Why are some sites so poorly predicted? Do they have very little data?

It looks like the maximum number of dates in a year ~ 49, which seems low. 
For Site CIK14, dates are approx. June to late July. Did data get clipped somewhere?

```{r}
spring_meanT %>% 
  count(Site, year) %>% 
  spread(key = year, value = n)

spring_meanT %>% 
  group_by(year) %>% 
  summarize(min(sampleDate),
            max(sampleDate))

```




```{r}
chosen.model <- lm(ma_mean ~ tair  + month*tair  + slope_P + elev_mean + cont_area + forest + shrub + wetland  , data = temp_mod,
                na.action = "na.fail")

summary(chosen.model)
plot(chosen.model)
summ(chosen.model,confint = TRUE, digits = 3)

#save predicted an residual
temp_mod$predicted <- predict(chosen.model)
temp_mod$residuals <- residuals(chosen.model)
hist(temp_mod$residuals)
ggplot(data = temp_mod, aes(x= ma_mean, y=predicted, colour=year)) + geom_point()
ggplot(data = temp_mod, aes(x= ma_mean, y=predicted, colour=Site)) + geom_point()

ggplot(data = temp_mod, aes(x= ma_mean, y=residuals, color=Site)) + geom_point()
ggplot(data = temp_mod, aes(x= day, y=residuals, group=year, color=year)) + geom_point()+ facet_wrap(~Site)
ggplot(data = temp_mod, aes(x= day, y=ma_mean, group=year, color=year)) + geom_point()+ facet_wrap(~Site)

#check out high residuals
resid <- temp_mod %>% filter(residuals > 3)
ggplot(data = resid, aes(x= ma_mean, y=residuals, color=Site)) + geom_point()
ggplot(data = resid, aes(x= ma_mean, y=predicted, color=Site)) + geom_point()

#star <- temp_mod %>% filter(Site=='STAR 171 Lower') 
#ggplot(data=resid, aes(y=ma_mean, x=sampleDate)) + geom_point()

```


# GAM models

Started by examining code used by Siegel and Volk in their paper and available on github.

https://github.com/SouthForkResearch/TemperatureModels_Predictive/blob/master/Model_fitting_ALL_1.R

Here is the code they used for spring and fall gam v. linear models:

```{r}

# m1gs <- (gam(AvgDailyTemp ~ s(TAVGn5dC, k = 4) +s(Tchange5C, k = 3)+s(A1Snow,by = JulianDate, k = 3)+s(catch_area, by = ae, k = 3)+s(catch_area, by = TAVGn5dC, k = 3)+s(catch_area, by = dailymeanCMS, k = 3)+s(ae, by = JulianDate, k = 3)+s(SLOPE, by = TAVGn5dC, k = 3)+s(Lake_perc, by = JulianDate, k = 3)+s(BFI, by = TAVGn5dC, k = 3)+ s(dailymeanCMS, by = TAVGn5dC, k = 3) + s(JulianDate, by = TAVGn5dC, k = 5)+ s(Echange2, by = TAVGn5dC, k = 3)+s(for_cover, by = TAVGn5dC, k = 3), data = data.day.sp))
# summary(m1gs)
# 
# #Spring linear
# m1gsL <- (lm(AvgDailyTemp ~ TAVGn5dC+Tchange5C+A1Snow*JulianDate+catch_area*ae+catch_area*TAVGn5dC+catch_area*dailymeanCMS+ae*JulianDate+SLOPE*TAVGn5dC+Lake_perc*JulianDate+dailymeanCMS*TAVGn5dC+JulianDate*TAVGn5dC+Echange2*TAVGn5dC+for_cover*TAVGn5dC, data = data.day.sp))
# summary(m1gsL)
# #fall model
# m1gf <- (gam(AvgDailyTemp ~ s(TAVGn3dC, k = 5) +s(Tchange3C, k = 3)+s(SNWD,by = TAVGn3dC, k = 3)+s(catch_area, by = ae, k = 3)+s(catch_area, by = dailymeanCMS, k = 3)+s(ae, by = SNWD, k = 3)+s(catch_area, by = TAVGn3dC, k = 3)+s(ae, by = TAVGn3dC, k = 3)+s(Lake_perc, by = JulianDate, k = 3)+ s(dailymeanCMS, by = TAVGn3dC, k = 3)+ s(BFI, by = TAVGn3dC, k = 3)+ s(Echange2, by = TAVGn3dC, k = 3)+ s(JulianDate, k = 5), data = data.day.fl))
# summary(m1gf)
# 
# m1gfL <- (lm(AvgDailyTemp ~ TAVGn3dC+Tchange3C+SNWD*TAVGn3dC+A1Snow*JulianDate+catch_area*ae+catch_area*TAVGn3dC+catch_area*dailymeanCMS+ae*JulianDate+ae*SNWD+SLOPE*TAVGn3dC+BFI*TAVGn3dC+Lake_perc*JulianDate+dailymeanCMS*TAVGn3dC+JulianDate*TAVGn3dC+Echange2*TAVGn3dC, data = data.day.fl))
# summary(m1gfL)

```

Try adding gam smoothers to the spring linear model that performed best. The main problem with this model is that we are seeing improved prediction with swe, but the effect size is in the wrong direction (+).

```{r}

spring.gam <- gam(meanT ~ s(tair, k = 3) + s(slope_P, k = 3) + s(cont_area, k = 3) +  s(prcp, k = 3) 
                  + s(swe, k = 3), data = spring_meanT)

summary(spring.gam)

sqrt(sum(((predict(spring.gam) - spring_meanT$meanT) ^ 2))/nrow(spring_meanT))

plot(spring.gam)
  

```

Compare to linear model.


```{r}

spring.lm <- lm(meanT ~ tair + slope_P + cont_area +  prcp + swe, data = spring_meanT)

summary(spring.lm)

sqrt(sum(((predict(spring.lm) - spring_meanT$meanT) ^ 2))/nrow(spring_meanT))

visreg(spring.lm, "tair")
visreg(spring.lm, "slope_P")
visreg(spring.lm, "cont_area")
visreg(spring.lm, "swe")
visreg(spring.lm, "prcp")

spring.resid <- lm(meanT ~ tair, data = spring_meanT) %>% resid
plot(spring_meanT$slope_P, spring.resid)
plot(spring_meanT$cont_area, spring.resid)
plot(spring_meanT$prcp, spring.resid)
plot(spring_meanT$swe, spring.resid)

```

Try a gam for the entire season.

```{r}
all.gam <- gam(meanT ~ s(tair, k = 5) + s(slope_P, k = 3) + s(cont_area, k = 3) +  s(prcp, k = 3) 
                  + s(sweA1, k = 3), data = temp_meanT)

summary(all.gam)

sqrt((sum((predict(all.gam) - temp_meanT$meanT) ^ 2))/nrow(temp_meanT))

plot(all.gam)
visreg(all.gam, "tair")


all.gam <- gam(meanT ~ s(tair, k = 5) + s(slope_P, k = 3) + s(cont_area, k = 3) + s(elev_mean, k = 3), data = temp_meanT)

summary(all.gam)

sqrt(sum(((predict(all.gam) - temp_meanT$meanT) ^ 2))/6049)


```

