
---
title: "1_Create model data frame"
author: "Becky"
date: "10/31/2019"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE)
library(lubridate)
library(rgdal)
# library(zoo)
# library(sf)
# library(corrplot)
# library(MuMIn)
# library(ape)
library(readxl)
library(tidyverse)
```

Merging all of the spatial and temporal covariates to the stream temperature data.

# Anchor Data

## Load and combined QAed datasets

Load data files from previously that only have useData == 1. 
Bind all the of data together and calculate mean daily temperatures.

```{r}
load("output/CIKdat_postQA.Rdata")
load("output/APUdat_postQA.Rdata")
load("output/KBRdat_postQA.Rdata")
temp <- tibble()
temp <- bind_rows(temp, CIKdatFinal) 
temp <- bind_rows(temp, KBRdatFinal) 
temp <- bind_rows(temp, APUdatFinal) 
#No NAs in temperature field so ok for calculating daily means
temp <- temp %>%
  group_by(Site, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) 
```

Plot to look at differences between sites.

```{r}

temp %>% 
  distinct(Site, year(sampleDate)) 

temp %>% 
  distinct(Site, year(sampleDate)) %>% 
  count(Site) %>% 
  arrange(n)

temp %>% 
  mutate(Year = year(sampleDate),
         mo_day = format(sampleDate, format = "%m-%d")) %>% 
  ggplot(aes(x = as.Date(mo_day, format = "%m-%d"), y = meanT, color = Site)) +
  geom_line() +
  facet_wrap(~Year) +
  theme(legend.position = "none")

```


Make a pdf of temperatures by site with different years to double check we don't have any major gw sites.

```{r}
sites <- temp %>% distinct(Site) %>% pull(Site)

pdf("output/Daily temps by year and site_qaed.pdf")
for(i in sites){
  dat <- temp %>% filter(Site == i)
  dat <- dat %>% 
    mutate(day = format(sampleDate, "%m-%d"),
           year = factor(year(sampleDate)))
  p <- ggplot(data = dat, aes(x = as.Date(day, format = "%m-%d"), y = meanT, color = year)) +
    # geom_point() +
    geom_line() +
    labs(title = i, x = "Date", y = "Mean Daily Temp.")
  print(p)
}
dev.off()

```





## Merge RCA ids to stream temperature data frame

RCA site ids can be used to filter the climate data actually needed for modeling.

Shifted a few sites so that they fell within the correct RCA (3 sites below the confluence of NF and SF Anchor). Also one of Sue's tributaries was shifted to be on the right stream and rca. Excel file in output folder that has site ids and RCAs linked. 

Note: the original accs_siteids applied to the KBR data weren't brought in because I had to go back to the raw data to figure out what was going on (site locations). So, create a join field that includes the contact_siteid for the KBR sites only. (contact id 2)

Also note: Some of the KBR data was not in the Anchor River watershed so they won't have rca_ids. Remove these below.

```{r}
rca_join <- read_excel("Data/anchor_sites_rcas.xlsx")
rca_join <- rca_join %>% 
  mutate(Site_join = case_when(contact_id == 2 ~ contact_siteid,
                               contact_id != 2 ~ accs_siteid))
rca_join
```

Join RCA IDs to the stream temperature data frame. Remove the data for the KBR sites from the Deep and Ninikchik watersheds - 8 sites. Note that there are RCAs for the Stariski sites.

```{r}
temp.rca <- rca_join %>% 
  dplyr::select(Site_join, rca_id) %>% 
  right_join(temp, by = c("Site_join" = "Site")) %>% 
  rename(Site = Site_join)
  
temp.rca %>% 
  distinct(Site, rca_id) %>% 
  arrange(rca_id)

temp.rca <- temp.rca %>% 
  filter(!rca_id == 0) %>% 
  mutate(year = year(sampleDate))
```

## Remove duplicate sites and years within an RCA

Check for duplicate sites and years within an rca.

* 248 
* 933
* 255
* 351
* 928

```{r}
temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  distinct(rca_id, Site, year) %>% 
  count(rca_id, year) %>% 
  arrange(desc(n))
```

RCA 248 has nanc-44 upper middle and lower in the same rca. Keep the lower site as long as it isn't missing data.

```{r}
temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  distinct(rca_id, Site, year) %>% 
  filter(rca_id == 248, year %in% c(2007, 2008))

temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  filter(rca_id == 248) %>% 
  count(Site, year)
```

Rca 933 had three sites with data from 2015 - drop apu1 and apu10. Sue's site has much more data for that year.

```{r}
temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  distinct(rca_id, Site, year) %>% 
  filter(rca_id == 933, year %in% c(2015))

temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  filter(rca_id == 933, year == 2015) %>% 
  count(Site, year)
```

Rca 255 has star-69 lower and middle. Keep lower, same amount of data and more representative of watershed.

```{r}
temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  distinct(rca_id, Site, year) %>% 
  filter(rca_id == 255, year %in% c(2007, 2008))

temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  filter(rca_id == 255) %>% 
  count(Site, year)
```

Rca 351 - star 171 lower and upper both have data for 2012/2013. Keep upper more representative.

```{r}
temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  distinct(rca_id, Site, year) %>% 
  filter(rca_id == 351, year %in% c(2012, 2013))

temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  filter(rca_id == 351) %>% 
  count(Site, year)
```

Rca 928 - sanc 1203 middle and upper. Keep middle both have same amount of data.

```{r}
temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  distinct(rca_id, Site, year) %>% 
  filter(rca_id == 928)

temp.rca %>% 
  mutate(year = year(sampleDate)) %>% 
  filter(rca_id == 928) %>% 
  count(Site, year)
```


Sites and years to drop because they are duplicates within an RCA:

* NANC-44-M 
* NANC-44-U
* APU10 in 2015
* APU1 in 2015
* STAR-69-M
* STAR 171 Lower
* SANC-1203-U

```{r}
temp.rca %>% distinct(Site)

temp.rca <- temp.rca %>% 
  filter(!Site %in% c("NANC-44-M", "NANC-44-U", "STAR-69-M", "STAR 171 Lower", "SANC-1203-U")) 

temp.rca <- temp.rca %>% 
  filter(!(Site %in% c("APU10", "APU1") & year == 2015)) 
```

For the remaining sites look at mean july temps by year to see if some of the little streams are really gw sites.

```{r}
temp.rca %>% 
  filter(month(sampleDate) == 7) %>% 
  group_by(Site, year) %>% 
  summarize(july = mean(meanT)) %>% 
  arrange(july)
```

Summary of final dataset.

```{r}
temp.rca %>% distinct(Site, year) %>% count(Site) %>% arrange(n)
```


## Merge rca and reach attributes to data frame

Read in covariates associated with the reaches and RCAs in the geodatabase. Note that the reachid and the rcaids are the same.

Everything should now be in two feature classes - one for rcas and one for reaches. The only data not quite there yet are the nlcd summaries so for now summarize from Dustin's table.

```{r}
fgdb <- "W:/GIS/Anchor/Anchor_spatial.gdb"
# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)
# Read the feature class
rca_reach <- sf::st_read(dsn = fgdb, layer = "anch_rca_reaches_attributed_03112020")
rcas <- sf::st_read(dsn = fgdb, layer = "anchor_rcas_attributed_06022020")
```

Summary of stream reach and rca attributes for report.

```{r}
rca_reach %>% 
  as.data.frame %>% 
  ggplot() +
  geom_histogram(aes(x = reach_slope))

rca_reach %>% 
  as.data.frame %>% 
  summary()

rca_reach %>% 
  as.data.frame %>% 
  pull(reach_slope) %>% 
  quantile(probs = c(0.05, 0.95))

rca_reach %>% 
  as.data.frame %>% 
  select(reach_length) %>% 
  mutate(rl_sort = sort(reach_length),
         rl_sum = cumsum(rl_sort)) %>% 
  slice(606)

rcas %>% 
  as.data.frame %>% 
  summary()

#total chinook habitat
rcas %>%
  as.data.frame() %>% 
  right_join(rca_reach %>% as.data.frame, by = c("rca_id" = "reach_id")) %>% 
  filter(K_r > 0|K_s > 0|K_p > 0) %>% 
  summarize(sum(reach_length)/1000)

rcas %>%
  as.data.frame() %>% 
  right_join(rca_reach %>% as.data.frame, by = c("rca_id" = "reach_id")) %>% 
  filter(CO_r > 0|CO_s > 0|CO_p > 0) %>% 
  summarize(sum(reach_length)/1000)
```



Merge attributes for all to the new data frame. Use the flowacc value and convert to square kilometers. (The cont_area is the flow accumulation multiplied by 25 and converted from square meters to square kilometers.)

```{r}

reach_dat <- rca_reach %>% 
  as.data.frame() %>% 
  dplyr::select(reach_id, reach_slope, reach_length, Forest, Shrub, Wetland, Riparian) 

temp.rca <- temp.rca %>% 
  left_join(reach_dat, by = c("rca_id" = "reach_id"))


rca_dat <- rcas %>% 
  as.data.frame() %>% 
  dplyr::select(rca_id:ca_elev_mn) %>% 
  mutate(cont_area = flowacc * 25 / 1000000)

temp.rca <- temp.rca %>% 
  left_join(rca_dat)

```

Create a new elevation covariate that matches what was used by Siegel and Volk - the difference between the mean elevation for the contributing area and the mean elevation for the rca (they used the site elevation, but it should be very comparable). 

```{r}
temp.rca <- temp.rca %>% 
  mutate(elev_delta = ca_elev_mn - rca_elev_mn)
```

## Merge daymet data to stream temperature data frame

Read in climate covariates stored in file geodatabases that were calculated for the RCAs. These are tables that have climate information linked to the RCAs. Start with the daymet data: air, precipitation, and SWE. We are now moving forward with 3-day averaged air temperatures. The daymet date represents the middle day.

(Previously, we were using 8-day averages to match the LST, but because daymet had a much stronger correlation with stream temperature, we are now just focussed on that dataset. In Siegel and Volk, they justified 3 day and 5 day lagged air temperatures so we are trying that out to match. Longer lags would be better for bigger systems.)

Feature classes in geodatabase with daymet data.

```{r}
fgdb <- "W:\\Leslie\\GIS\\KFHP\\Geodatabase\\anchor_DAYMET.gdb"
# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)
```

Read in the feature class with 3-day air temperatures.

```{r}
dm3day <- sf::st_read(dsn = fgdb, layer = "tair3_anchor_all")
dm3day <- dm3day %>% 
  mutate(date = as.Date(date)) %>% 
  dplyr::select(-Field1)
```

Read in 5-day summed precipitation. Middle day of 5-day window (leslie used zoo::rollapply with default settings which is align=center), need to add TWO days before merging so that 5-day sum is 4 days previous + day of stream temperature.

```{r}
prcp5day <- sf::st_read(dsn = fgdb, layer = "prcp5day")
prcp5day <- prcp5day %>% 
  mutate(date = as.Date(date)) %>% 
  dplyr::select(-Field1)
```

Merge the daymet data (i.e. air temp, swe, and precip covariates) to the stream temperature data frame so that only rca_ids and dates in the stream temperature data frame for which we have empirical data are kept. For now, remove daymet so it doesn't bog down the system (though we will need it later for prediction). 

Note: moving average of stream temperatures is currently indexed by last day of 8 day window. Daymet is middle of 3-day window. We want daymet to represent day of and two days prior for modeling daily stream temperatures.

Call this data frame temp.dm since it will be our model using the daymet air temperatures. (next model can be temp.lst.)

```{r}
dm3day <- dm3day %>% 
  mutate(date_day3 = date + 1) %>% 
  dplyr::select(-date)

temp.dm <- temp.rca %>% 
  left_join(dm3day, by = c("rca_id" = "rca_id", "sampleDate" = "date_day3")) 

# rm(dm3day)
```




Bring in 5 day precipitation. 

```{r}
prcp5day <- prcp5day %>% 
  mutate(date_day5 = date + 2) %>% 
  dplyr::select(-date)

temp.dm <- temp.dm %>% 
  left_join(prcp5day, by = c("rca_id" = "rca_id", "sampleDate" = "date_day5")) 
 
```


Figure out pattern in missing data. All from Jan-April and Oct-Dec. We can drop those dates because we are only modeling summer stream temperatures.

```{r}
temp.dm %>% 
  filter(is.na(tair3)) %>% 
  distinct(month(sampleDate))
```

Remove fall/winter stream temps that we don't plan to model. 

```{r}
temp.dm <- temp.dm %>% 
  filter(!is.na(tair3)) 
```

Add columns for week, month, and year.

```{r}
temp.dm <- temp.dm %>% 
  mutate(week = week(sampleDate),
         month = month(sampleDate),
         year = year(sampleDate),
         day = as.numeric(format(sampleDate, "%j"))) %>% 
  arrange(Site, sampleDate)
  
```




## Merge April 1st swe from Daymet

Leslie added a file to the data folder with this information.

```{r}
sweA1 <- read_csv("Data/sweA1_anchor.csv")

sweA1 <- sweA1 %>% 
  mutate(year = year(date)) %>% 
  dplyr::select(rca_id, sweA1, year)

temp.dm <- temp.dm %>% 
  left_join(sweA1, by=c("rca_id", "year"))
```



## Temperature Index 

Remove temperatures less than 0 and calculate temperature index (waterT-airT)/waterT. index approaches 0 then air and stream temp are most similar.
 
```{r, echo=FALSE, warnings = FALSE}
temp.dm %>% 
  filter(meanT < 0) %>% 
  distinct(Site, month, day, meanT)

temp.dm <- temp.dm %>% 
  filter(meanT > 0) %>% 
  mutate(index = (meanT - tair3)/meanT)
```

Explore temperature index by sites and year. All bad values are early. 

``` {r, echo=FALSE, warnings = FALSE}

ggplot(data=temp.dm, aes(x = day, y = index, color=factor(year))) + 
  geom_point () +
  facet_wrap(~Site) 

```

Save this version of the data frame for showing the temp index values in May for the report.

```{r}
temp.dm.may <- temp.dm

save(temp.dm.may, file = "output/temp.dm.may.Rdata")
```

Remove all data prior to June 1 (Julian Day 152) and replot index

```{r, warnings = FALSE, echo=FALSE}
temp.dm <- temp.dm %>% 
  filter(day > 152) 
```


```{r, warnings = FALSE, echo=FALSE}
ggplot(data=temp.dm, aes(x = day, y = index, color=factor(year))) + 
  geom_point () +
  facet_wrap(~Site) 

```

## Seasonal split on maximum temperatures

Identify median value of Maximum stream temperature and air temperature to locate apex of ascending and descending limb and seasonal break in model. Stream temp = 201 and air temp = 205. Choose July 20 (julian day = 201) as split. Will have 2 seasons June 1 - July 20: (152-201) and july 21 - September 30: (202-303)

```{r}
sites <- temp %>% distinct(Site) %>% pull(Site)
# i <- sites[14]
#pdf("Plots of temperature by year and site.pdf", width = 12, height = 8, units = "in")
for(i in sites) {
  dat <- temp %>% filter(Site == i)
  sitename <- dat %>% distinct(Site) %>% pull(Site)
  p <- ggplot(data = dat, aes(x = day, y = ma_mean)) +
    geom_line() +
    facet_wrap(~year) +
    labs(main = sitename)
  print(p)
}
dev.off()
#get median julian day for max ma_mean
max_ma_mean <- temp %>% 
  filter(month %in% 6:8) %>% 
  group_by(Site, year) %>% 
  summarize(ma_mean = max(ma_mean, na.rm = TRUE),
            number_of_days = n()) %>% 
  filter(number_of_days > 71)
max_ma_mean
temp %>% 
  right_join(max_ma_mean) %>% 
  summarize(mean(day),
            sd(day))
#get median julian day for air temperature
max_tair <- temp %>% 
  filter(month %in% 6:8) %>% 
  group_by(Site, year) %>% 
  summarize(max_tair = max(tair, na.rm = TRUE),
            number_of_days = n()) %>% 
  filter(number_of_days > 71)
max_tair
temp %>% 
  right_join(max_tair) %>% 
  summarize(mean(day),
            sd(day))
```




## Save data frame with empirical data, daymet, and lst.

Check that everything is numeric and fix those that are not. Add variable for season so can run a full model with different slopes for ascending and descending limbs of temperature. season*tair3

```{r}
str(temp.dm)

temp.dm <- temp.dm %>% 
  mutate(Riparian = as.numeric(Riparian),
         season = case_when(day < 202 ~ "spring",
                            day > 201 ~ "fall"))

temp.dm %>% distinct(season, day)

save(temp.dm, file = "output/temp.dm.Rdata")
```

## Create larger data frame with 8-day daymet and lst

### 8-day moving average of stream temperature data: NOT RUN BC USING DAYMET ONLY NOW.

Moving average function in CAtools which will remove NAs. We can now calculate a running mean independent of the number of data points available in each window (e.g. 8). Separately, calculate the number of non NA values in each window by replacing all non-NA values with 1 and filter on those running means with counts < 5.

Use complete to fill in missing dates for all sites so that moving average is only calculating when the 8 days prior actually have data. (i.e. not going from October 2004 to April 2005).

Calculate a moving average that takes the 7 days prior and day itself and calculates a mean and removes any missing values (NAs). Add a new field that is 0 when mean temp is NA and 1 when we have a measurement. Do a rolling sum on that field over the same window so we know how many measurements were used in the rolling mean. Only keep moving averages that were calculated from 5 or more measurements! DONE!

```{r}
all.dates <- tibble(sampleDate = seq.Date(from = min(temp.dm$sampleDate), 
                                          to = max(temp.dm$sampleDate), by = 1)) 

temp_8day <- temp.dm %>%
  complete(Site, all.dates) %>% 
  arrange(Site, sampleDate) %>% 
  #add a field that can be used to count non-NA values
  mutate(meanNotNA = case_when(is.na(meanT) ~ 0,
                               TRUE ~ 1)) %>% 
  group_by(Site) %>% 
  mutate(ma_mean = rollapply(meanT, width = 8, align = "right", 
                             FUN = function(x) mean(x, na.rm = TRUE), fill = NA),
         ma_sum = rollapply(meanNotNA, width = 8, align = "right", 
                            FUN = sum, fill = NA)) %>% 
  filter(!ma_sum < 5)

```

Are there missing data in temp_8day? Only for meanT, ma_mean is all filled in. When we are filling in dates and accepting moving averages with 5 or more measurements, we are extending the data series out by three days beyond the last measurement. We can delete these or accept them. For now, leave them in.

```{r}
summary(temp_8day)
temp_8day %>% filter(is.na(meanT))
```

Plot of mean temps and moving average.

```{r}
temp_8day %>% 
  filter(Site == "CIK8") %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanT)) +
  geom_line(aes(x = sampleDate-4, y = ma_mean), color = "red") +
  facet_wrap(~Site)
```

Merge the 8 day moving averages back to the temp.dm data.frame. BUT rename. This is a more complete data frame for justifying NOT using lst or 8-day averages in next script of data and model exploration.

```{r}
temp.dm2 <- left_join(temp.dm, temp_8day %>% select(Site, sampleDate, ma_mean))

```

Read in the date with 8-day air. Just focus on air for model comparison. There was also originally and 8-day precip and 8-day swe. For precip, I'm not sure if that was a sum or an average. And for swe, we decided an annual index of snowpack was better.

```{r}
fgdb <- "W:\\Leslie\\GIS\\KFHP\\Geodatabase\\anchor_DAYMET.gdb"
# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)
```


```{r}
dm8day <- sf::st_read(dsn = fgdb, layer = "tair_anchor_all")
dm8day <- dm8day %>%
  mutate(date = as.Date(date)) %>%
  select(-Field1)
```

Since now modeling daily stream temperatures, change the date so that it represents the last day of the 8-day period.

From Leslie: "For the daymet data the date represents the mid-point of the 8-day series. For May 1st - average is 4 days prior and 3 days after." Email on 10/18/19.

```{r}
dm8day <- dm8day %>%
  mutate(date_day8 = date + 3)
temp.dm2 <- temp.dm2 %>%
  left_join(dm8day %>% select(-date),
            by = c("rca_id" = "rca_id", "sampleDate" = "date_day8"))

```

## Merge LST data to data frame

Read in LST data.

```{r}
fgdb <- "W:/Leslie/GIS/KFHP/Geodatabase/anchor_LST.gdb"

# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)

# Read the feature class
lst <- sf::st_read(dsn = fgdb, layer = "LST_anchor_all")
lst <- lst %>%
  mutate(date = as.Date(date)) %>%
  select(-Field1)
```

Merge LST to the model data frame with daymet and spatial covariates. Leslie and Timm clarified what the date represents for LST in an email on 10/18/19: "The date in the LST file name represents the first day of a consecutive 8 day period". Add a date to match the 8 day moving average of stream temperature before merging. This will be the last day in the 8-day window and so can now be used to model daily mean temperatures.

```{r}
lst <- lst %>%
  mutate(date_day8 = date + 7,
         lst = LST * 0.02 - 273.15)
temp.dm2 <- temp.dm2 %>%
  left_join(lst %>% select(-date, -LST),
            by = c("rca_id" = "rca_id", "sampleDate" = "date_day8"))

```

Save this data frame for comparing different air temperature covariates in next markdown: 8day daymet versus 8 day lst, 8-day daymet vs. 3-day daymet.

```{r}
save(temp.dm2, file = "output/temp.dm2.Rdata")
```




## STOPPED HERE NOTHING BELOW HAS BEEN RUN 2/5/20

## Data availability by site and year table

Number of sites within each year and number of days within each year. (similar to table 1 in mcnyset)

```{r}
temp.dm %>% 
  distinct(sampleDate, year) %>% 
  count(year)
temp.dm %>% 
  distinct(Site, year) %>% 
  count(year)
```

## Exploring options for spatial and temporal autocorrelation

Check for spatial autocorrelation in residuals. Following instructions here: [How can I calculate Moran's I in R?] https://stats.idre.ucla.edu/r/faq/how-can-i-calculate-morans-i-in-r/.


```{r}
temp.dm <- temp.dm %>% 
  mutate(resid.fm = resid(full.model)) 
temp.dists <- temp.dm %>% 
  left_join(rca_join %>% select(Site_join, latitude, longitude), by = c("Site" = "Site_join")) 
temp.dists <- temp.dists %>% 
  filter
  select(latitude, longitude) %>% 
  dist(.) %>% 
  as.matrix
temp.dists.inv <- 1/(temp.dists + 1)
diag(temp.dists.inv) <- 0
 
temp.dists.inv[1:5, 1:5]
temp.dists[1:50, 1:50]
Moran.I(temp.dm$resid.fm, temp.dists.inv)
```

I don't think this is working because we have very few sites, ~ 20, but 8000 observations. This may not be a meaningful thing to look at, except for when we have data from many sites on the same date.
Pick a sample date from 2007 with all 15 sites, sometime in September. Looks ok. Could loop this to calculate for a series of dates with many sites (15 or more)....

```{r}
temp.dm %>% 
  distinct(Site, sampleDate) %>% 
  count(sampleDate) %>% 
  filter(n > 14)
temp.dists <- temp.dm %>% 
  filter(sampleDate == "2007-09-20") %>% 
  left_join(rca_join %>% select(Site_join, latitude, longitude), by = c("Site" = "Site_join")) 
temp.dists <- temp.dists %>% 
  select(latitude, longitude) %>% 
  dist(.) %>% 
  as.matrix
temp.dists.inv <- 1/(temp.dists)
diag(temp.dists.inv) <- 0
 
temp.dists.inv[1:5, 1:5]
Moran.I(temp.dm %>% filter(sampleDate == "2007-09-20") %>% pull(resid.fm), temp.dists.inv)
```


## Checking for temporal autocorrelation.

We will need to fill all the missing dates back in for each site and check by Site. The function to fill in dates by group is above.

## Save CSV of all data currently available 12.20.19

```{r}
# write.csv(temp, "output/temp_model_data_122019.csv")
```
