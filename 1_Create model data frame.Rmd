---
title: "1_Create model data frame"
author: "Becky"
date: "10/31/2019"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE)

library(lubridate)
library(rgdal)
library(zoo)
library(sf)
library(corrplot)
library(MuMIn)
library(ape)
library(readxl)
library(tidyverse)
```

Merging all of the spatial and temporal covariates to the stream temperature data.

# Anchor Data

## 8-day moving average of stream temperature data

Moving average function in CAtools which will remove NAs. We can now calculate a running mean independent of the number of data points available in each window (e.g. 8). Separately, calculate the number of non NA values in each window by replacing all non-NA values with 1 and filter on those running means with counts < 5.

Load data files from previously that only have useData == 1. 
Bind all the of data together first before calculating running means. Calculate mean daily temperatures.

```{r}

load("output/CIKdat_postQA.Rdata")
load("output/APUdat_postQA.Rdata")
load("output/KBRdat_postQA.Rdata")

temp <- as_tibble()

temp <- bind_rows(temp, CIKdatFinal) 
temp <- bind_rows(temp, KBRdatFinal) 
temp <- bind_rows(temp, APUdatFinal) 

#No NAs in temperature field so ok for calculating daily means
temp <- temp %>%
  group_by(Site, sampleDate) %>% 
  summarize(meanT = mean(Temperature)) 


```


Use complete to fill in missing dates for all sites so that moving average is only calculating when the 8 days prior actually have data. (i.e. not going from October 2004 to April 2005).

Calculate a moving average that takes the 7 days prior and day itself and calculates a mean and removes any missing values (NAs). Add a new field that is 0 when mean temp is NA and 1 when we have a measurement. Do a rolling sum on that field over the same window so we know how many measurements were used in the rolling mean. Only keep moving averages that were calculated from 5 or more measurements! DONE!

```{r}

all.dates <- tibble(sampleDate = seq.Date(from = min(temp$sampleDate), 
                                          to = max(temp$sampleDate), by = 1)) 

temp_8day <- temp %>%
  complete(Site, all.dates) %>% 
  arrange(Site, sampleDate) %>% 
  #add a field that can be used to count non-NA values
  mutate(meanNotNA = case_when(is.na(meanT) ~ 0,
                               TRUE ~ 1)) %>% 
  group_by(Site) %>% 
  mutate(ma_mean = rollapply(meanT, width = 8, align = "right", 
                             FUN = function(x) mean(x, na.rm = TRUE), fill = NA),
         ma_sum = rollapply(meanNotNA, width = 8, align = "right", 
                            FUN = sum, fill = NA)) %>% 
  filter(!ma_sum < 5)

```

Are there missing data in temp_8day? Only for meanT, ma_mean is all filled in. When we are filling in dates and accepting moving averages with 5 or more measurements, we are extending the data series out by three days beyond the last measurement. We can delete these or accept them. For now, leave them in.

```{r}
summary(temp_8day)

temp_8day %>% filter(is.na(meanT))
```

Plot of mean temps and moving average.

```{r}

temp_8day %>% 
  filter(Site == "CIK8") %>% 
  ggplot() +
  geom_line(aes(x = sampleDate, y = meanT)) +
  geom_line(aes(x = sampleDate-4, y = ma_mean), color = "red") +
  facet_wrap(~Site)
```


## Merge RCA ids to stream temperature data frame

RCA site ids can be used to filter the climate data actually needed for modeling.

Shifted a few sites so that they fell within the correct RCA (3 sites below the confluence of NF and SF Anchor). Also one of Sue's tributaries was shifted to be on the right stream and rca. Excel file in output folder that has site ids and RCAs linked. 

Note: the original accs_siteids applied to the KBR data weren't brought in because I had to go back to the raw data to figure out what was going on (site locations). So, create a join field that includes the contact_siteid for the KBR sites only. (contact id 2)

Also note: Some of the KBR data was not in the Anchor River watershed so they won't have rca_ids. Remove these below.

```{r}

rca_join <- read_excel("Data/anchor_sites_rcas.xlsx")

rca_join <- rca_join %>% 
  mutate(Site_join = case_when(contact_id == 2 ~ contact_siteid,
                               contact_id != 2 ~ accs_siteid))

rca_join
```

Join RCA IDs to the stream temperature data frame. Remove the data for the KBR sites from the Deep and Ninikchik watersheds - 8 sites. Note that there are RCAs for the Stariski sites.

```{r}

temp.rca <- rca_join %>% 
  select(Site_join, rca_id) %>% 
  right_join(temp_8day, by = c("Site_join" = "Site")) %>% 
  rename(Site = Site_join)
  
temp.rca %>% 
  distinct(Site, rca_id) %>% 
  arrange(rca_id)

temp.rca <- temp.rca %>% 
  filter(!rca_id == 0)
```


## Merge rca and reach attributes to data frame

Read in covariates associated with the reaches and RCAs in the geodatabase. Note that the reachid and the rcaids are the same.

Everything should now be in two feature classes - one for rcas and one for reaches. The only data not quite there yet are the nlcd summaries so for now summarize from Dustin's table.

```{r}

fgdb <- "Data/Spatial_data/Anchor/KFHP/Geodatabases/Anchor.gdb"

# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)

# Read the feature class
rca_reach <- sf::st_read(dsn = fgdb, layer = "anch_rca_reaches")

rcas <- sf::st_read(dsn = fgdb, layer = "anch_rca")

```

Merge attributes for all to the new data frame. Use the flowacc value and convert to square kilometers. (The cont_area is the flow accumulation multiplied by 25 and converted from square meters to square kilometers.)

```{r}

temp.rca <- temp.rca %>% 
  left_join(rca_reach %>% select(reachid, reach_slope, reach_length), by = c("rca_id" = "reachid")) %>% 
  select(-Shape)

temp.rca <- temp.rca %>% 
  left_join(rcas %>% select(rca_id:S_p, HUC_name:ca_elev_mn)) %>% 
  select(-Shape) %>% 
  mutate(cont_area = flowacc * 25 / 1000000)

```

Create a new elevation covariate that matches what was used by Siegel and Volk - the difference between the mean elevation for the contributing area and the mean elevation for the rca (they used the site elevation, but it should be very comparable). 

```{r}

temp.rca <- temp.rca %>% 
  mutate(elev_delta = ca_elev_mn - rca_elev_mn)

```

Read in the NLCD attributes - percent forest, wetland, and shrub in the riparian buffer of each reach. Dustin is working on getting these attributes on the reaches feature class, so later these can be read in there.

I calculated percent wetland, forest, and shrub riparian cover in GIS by buffering stream by 30m, converted nlcd raster to polygon, used select function to save individual polygon layers of wetland, forest, and shrub. Used Tabulate intersection function to get tables by rca_id or percent overlap and then joined tables to rca shapefile.
The covariates are joined individually to the anchor dataframe here. Code also replaces NA with 0 for all 3 covariates.

```{r}

nlcd <- sf::st_read(dsn = fgdb, layer = "anch_rca_reaches_NLCD_Tab_Int")

buffer_summs <- nlcd %>% 
  filter(Land_Cover_Reclass %in% c("Forest", "Shrub", "Wetlands")) %>% 
  group_by(reachid, Land_Cover_Reclass) %>% 
  summarize(percent_cover = sum(PERCENTAGE)) %>% 
  rename(rca_id = reachid) %>% 
  spread(key = Land_Cover_Reclass, value = percent_cover, fill = 0)

temp.rca <- temp.rca %>% 
  left_join(buffer_summs)

```

## STILL NEED TO ADD IN PRECIP AND SWE FROM DAYMET - SKIPPING FOR NOW AND MOVING FORWARD WITH 3DAY AIR

## Merge daymet data to stream temperature data frame

Read in climate covariates stored in file geodatabases that were calculated for the RCAs. These are tables that have climate information linked to the RCAs.

Start with the daymet data: air, precipitation, and SWE. We are now moving forward with 3-day averaged air temperatures. The daymet date represents the middle day.

NOTE: currently, the precip and swe are still 8-day averages, unchanged. Leslie is planning to sum 3 day precipitation by RCA and add that in later.

(Previously, we were using 8-day averages to match the LST, but because daymet had a much stronger correlation with stream temperature, we are now just focussed on that dataset. In Siegel and Volk, they justified 3 day and 5 day lagged air temperatures so we are trying that out to match. Longer lags would be better for bigger systems.)



```{r}

fgdb <- "Data/Spatial_data/Anchor/KFHP/Geodatabases/anchor_DAYMET.gdb"

# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)

# Read the feature class
daymet <- sf::st_read(dsn = fgdb, layer = "tair3_anchor_all")

daymet <- daymet %>% 
  mutate(date = as.Date(date)) %>% 
  select(-Field1)

daymet %>% 
  group_by(year(date)) %>% 
  summarize(min(date), max(date))


```

Merge the daymet data (i.e. air temp, swe, and precip covariates) to the stream temperature data frame so that only rca_ids and dates in the stream temperature data frame for which we have empirical data are kept. For now, remove daymet so it doesn't bog down the system (though we will need it later for prediction). 

Note: moving average of stream temperatures is currently indexed by last day of 8 day window. Daymet is middle of 3-day window. We want daymet to represent day of and two days prior for modeling daily stream temperatures.

Call this data frame temp.dm since it will be our model using the daymet air temperatures. (next model can be temp.lst.)

```{r}

daymet <- daymet %>% 
  mutate(date_day3 = date + 1)

temp.dm <- temp.rca %>% 
  left_join(daymet, by = c("rca_id" = "rca_id", "sampleDate" = "date_day3")) 

# rm(daymet)
```

Figure out pattern in missing data. All from Jan-April and Oct-Dec. We can drop those dates because we are only modeling summer stream temperatures.

```{r}
temp.dm %>% 
  filter(is.na(tair3)) %>% 
  distinct(month(sampleDate))
```

Remove fall/winter stream temps that we don't plan to model. 

```{r}
temp.dm <- temp.dm %>% 
  filter(!is.na(tair3)) 
```

Add columns for week, month, and year.

```{r}
temp.dm <- temp.dm %>% 
  mutate(week = week(sampleDate),
         month = month(sampleDate),
         year = year(sampleDate),
         day = as.numeric(format(sampleDate, "%j"))) %>% 
  arrange(Site, sampleDate)
  
```


## Merge April 1st swe from Daymet

Leslie added a file to the data folder with this information.

```{r}

sweA1 <- read_csv("Data/sweA1_anchor.csv")
sweA2 <- sweA1 %>% mutate(date = as.Date(date), sampyear= year(date)) %>% select(rca_id, sweA1, sampyear)

temp.dm <- temp.dm6 %>% left_join(sweA2, by=c("rca_id", "sampyear"))

```


## Merge LST data to data frame

Read in LST data.

```{r}
fgdb <- "Data/Spatial_data/Anchor/KFHP/Geodatabases/anchor_LST.gdb"

# List all feature classes in a file geodatabase
subset(ogrDrivers(), grepl("GDB", name))
fc_list <- ogrListLayers(fgdb)
print(fc_list)

# Read the feature class
lst <- sf::st_read(dsn = fgdb, layer = "LST_anchor_all")

lst <- lst %>% 
  mutate(date = as.Date(date)) %>% 
  select(-Field1)

```

Merge LST to the model data frame with daymet and spatial covariates. Leslie and Timm clarified what the date represents for LST in an email on 10/18/19: "The date in the LST file name represents the first day of a consecutive 8 day period..". Add a date to match the 8 day moving average of stream temperature before merging.

```{r}

lst <- lst %>% 
  mutate(date_day8 = date + 7)


temp.dm <- temp.dm %>% 
  left_join(lst, by = c("rca_id" = "rca_id", "sampleDate" = "date_day8")) %>% 
  mutate(lst2 = LST * 0.02 - 273.15)

```


 - 

## Temperature Index 

Remove temperatures less than 0 and calculate temperature index (waterT-airT)/waterT. index approaches 0 then air and stream temp are most similar.
 
```{r, echo=FALSE, warnings = FALSE}
#filter temps less than 2
temp <- temp.dm %>% filter(ma_mean > 0) %>% mutate(index=(ma_mean-tair)/ma_mean)

ggplot() + geom_point (data=temp, aes(x = day, y = index, color=year)) +
     facet_wrap(~Site, scales="free_y") 

month5b <- filter(temp,month==5) 
ggplot() + geom_point (data=month5b, aes(x = day, y = index, color=year)) +
     facet_wrap(~Site, scale="free_y") 

month6b <- filter(temp,month==6) 
ggplot() + geom_point (data=month6b, aes(x = day, y = index, color=year)) +
     facet_wrap(~Site, scales="free_y") 

month7b <- filter(temp,month==7) 
ggplot() + geom_point (data=month7b, aes(x = day, y = index, color=year)) +
     facet_wrap(~Site, scales="free_y") 

month8b <- filter(temp,month==8) 
ggplot() + geom_point (data=month8b, aes(x = day, y = index, color=year)) +
     facet_wrap(~Site, scales="free_y") 

month9b <- filter(temp,month==9) 
ggplot() + geom_point (data=month9b, aes(x = day, y = index, color=year)) +
     facet_wrap(~Site, scales="free_y") 



```
### Remove all data prior to June 1 (Julian Day 152) and replot index
```{r, warnings = FALSE, echo=FALSE}
temp <- temp %>% filter(day > 152) 
ggplot() + geom_point (data=temp, aes(x = day, y = index, color=year)) +
     facet_wrap(~Site)
```


### plot time-series to see seasonal split
Identify median value of Maximum stream temperature and air temperature to locate apex of ascending and descending limb and seasonal break in model. Stream temp = 201 and air temp = 205. Choose July 20 (julian day = 201) as split. Will have 2 seasons June 1 - July 20: (152-201) and july 21 - September 30: (202-303)

```{r}

sites <- temp %>% distinct(Site) %>% pull(Site)
# i <- sites[14]

#pdf("Plots of temperature by year and site.pdf", width = 12, height = 8, units = "in")

for(i in sites) {
  dat <- temp %>% filter(Site == i)
  sitename <- dat %>% distinct(Site) %>% pull(Site)
  p <- ggplot(data = dat, aes(x = day, y = ma_mean)) +
    geom_line() +
    facet_wrap(~year) +
    labs(main = sitename)
  print(p)
}

dev.off()

#get median julian day for max ma_mean
max_ma_mean <- temp %>% 
  filter(month %in% 6:8) %>% 
  group_by(Site, year) %>% 
  summarize(ma_mean = max(ma_mean, na.rm = TRUE),
            number_of_days = n()) %>% 
  filter(number_of_days > 71)

max_ma_mean

temp %>% 
  right_join(max_ma_mean) %>% 
  summarize(mean(day),
            sd(day))

#get median julian day for air temperature
max_tair <- temp %>% 
  filter(month %in% 6:8) %>% 
  group_by(Site, year) %>% 
  summarize(max_tair = max(tair, na.rm = TRUE),
            number_of_days = n()) %>% 
  filter(number_of_days > 71)

max_tair

temp %>% 
  right_join(max_tair) %>% 
  summarize(mean(day),
            sd(day))

```




## Save data frame with empirical data, daymet, and lst.

```{r}
save(temp.dm, file = "output/temp.dm")

```

## STOPPED HERE NOTHING BELOW HAS BEEN RUN 2/5/20

## Data availability by site and year table

Number of sites within each year and number of days within each year. (similar to table 1 in mcnyset)

```{r}
temp.dm %>% 
  distinct(sampleDate, year) %>% 
  count(year)

temp.dm %>% 
  distinct(Site, year) %>% 
  count(year)
```

## Exploring options for spatial and temporal autocorrelation

Check for spatial autocorrelation in residuals. Following instructions here: [How can I calculate Moran's I in R?] https://stats.idre.ucla.edu/r/faq/how-can-i-calculate-morans-i-in-r/.


```{r}
temp.dm <- temp.dm %>% 
  mutate(resid.fm = resid(full.model)) 

temp.dists <- temp.dm %>% 
  left_join(rca_join %>% select(Site_join, latitude, longitude), by = c("Site" = "Site_join")) 

temp.dists <- temp.dists %>% 
  filter
  select(latitude, longitude) %>% 
  dist(.) %>% 
  as.matrix

temp.dists.inv <- 1/(temp.dists + 1)
diag(temp.dists.inv) <- 0
 
temp.dists.inv[1:5, 1:5]

temp.dists[1:50, 1:50]

Moran.I(temp.dm$resid.fm, temp.dists.inv)
```

I don't think this is working because we have very few sites, ~ 20, but 8000 observations. This may not be a meaningful thing to look at, except for when we have data from many sites on the same date.
Pick a sample date from 2007 with all 15 sites, sometime in September. Looks ok. Could loop this to calculate for a series of dates with many sites (15 or more)....

```{r}
temp.dm %>% 
  distinct(Site, sampleDate) %>% 
  count(sampleDate) %>% 
  filter(n > 14)

temp.dists <- temp.dm %>% 
  filter(sampleDate == "2007-09-20") %>% 
  left_join(rca_join %>% select(Site_join, latitude, longitude), by = c("Site" = "Site_join")) 

temp.dists <- temp.dists %>% 
  select(latitude, longitude) %>% 
  dist(.) %>% 
  as.matrix

temp.dists.inv <- 1/(temp.dists)
diag(temp.dists.inv) <- 0
 
temp.dists.inv[1:5, 1:5]

Moran.I(temp.dm %>% filter(sampleDate == "2007-09-20") %>% pull(resid.fm), temp.dists.inv)


```


## Checking for temporal autocorrelation.

We will need to fill all the missing dates back in for each site and check by Site. The function to fill in dates by group is above.

## Save CSV of all data currently available 12.20.19

```{r}
write.csv(temp.dm, "output/temp_model_data_122019.csv")
save(temp.dm, file = "output/temp_model_data.Rdata")

```



